\documentclass[bsc,frontabs,singlespacing,parskip,deptreport]{infthesis}

\usepackage[round]{natbib}
\usepackage[hidelinks,colorlinks,allcolors=blue]{hyperref}

\usepackage{graphicx}

\usepackage{textcomp} % for text tilde
\def\mytilde{{\raise.17ex\hbox{$\scriptstyle\sim$}}}

% bold in math mode, making the~needed fonts available
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{bm}

\usepackage{wasysym}
\usepackage{listings}
\usepackage{spverbatim}

% cross and tick symbols
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% additional keywords in math mode
\DeclareMathOperator{\softmax}{softmax}

% Nice references including the~words Figure, Section, etc
\renewcommand*{\chapterautorefname}{Chap.}
\renewcommand*{\sectionautorefname}{Sec.}
\renewcommand*{\subsectionautorefname}{Sec.}
\renewcommand*{\subsubsectionautorefname}{Sec.}
\renewcommand*{\figureautorefname}{Fig.}
\renewcommand*{\tableautorefname}{Tab.}
\newcommand{\algorithmautorefname}{Alg.}
\def\equationautorefname~#1\null{(#1)\null}

% Change font family and size in captions
\usepackage[labelfont=bf,justification=justified,format=plain]{caption}
\DeclareCaptionFont{captionfont}{\small\fontseries{n}\fontfamily{phv}\selectfont}
\captionsetup[table]{font=captionfont}
\captionsetup[figure]{font=captionfont}

%
% TABLES
%
% Fancy things in tables
\usepackage{booktabs}
\usepackage{array}
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}} % left-aligned, fixed-width table cells
\usepackage{hhline} % double hline
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{color, colortbl} % coloured background in tables
\definecolor{CoLA}{RGB}{0, 173, 118}
\definecolor{SST-2}{RGB}{191, 115, 0}
\definecolor{Sara}{RGB}{90, 23, 238}
\usepackage{multirow} % multi-row and multi-column table cells
\usepackage{makecell} % line breaks inside cells with \thead{} and \makecell{}
\usepackage{longtable} % allow table to span multiple pages

% Globally setting the~vertical padding in tables
\renewcommand{\arraystretch}{1.1}

% Spacing around lines in tables
\def\abovestrut#1{\rule[0in]{0in}{#1}\ignorespaces}
\def\belowstrut#1{\rule[-#1]{0in}{#1}\ignorespaces}
\def\abovespace{\abovestrut{0.17in}}
\def\aroundspace{\abovestrut{0.17in}\belowstrut{0.10in}}
\def\belowspace{\belowstrut{0.10in}}


% For minipages with multiple figures, each with its own subcaption
\usepackage{subcaption}

% For verbatim in captions
\usepackage{cprotect}

% For verbatim in footnotes
\usepackage{fancyvrb}

% redefine \VerbatimInput
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
{
 fontsize=\footnotesize,
 frame=lines,  % top and bottom rule only
 framesep=2em, % separation between frame and text
 % label=\fbox{\color{Black}},
 % labelposition=all,
 }

% Background colour for text
\def\reviewready{\colorbox{yellow}{[READY FOR REVIEW]}}
\def\reviewed{\colorbox{green}{[REVIEWED]}}

% shortcuts for obscure, frequently used expressions
\def\BERTT{BERT\textsubscript{T}}
\def\BERTS{BERT\textsubscript{S}}
\def\LSTMS{LSTM\textsubscript{S}}
\def\sliding{The~lines are smoothed using sliding average with a~2-epochs-wide window.}

% Circled numbers in text
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}

% URLs without https://
\newcommand\rurl[1]{%
  \href{https://#1}{\nolinkurl{#1}}%
}

% TO-DO: Change .png figures to .pdf.
\begin{document}
  \colorlet{CoLA-l}{white!80!CoLA}
  \colorlet{SST-2-l}{white!80!SST-2}
  \colorlet{Sara-l}{white!80!Sara}

\VerbatimFootnotes

\title{
  \vspace{-5.0cm} \centering{\includeshield} \vspace{1cm} \\ 
  Teacher-student knowledge distillation from BERT
}

\author{Sam Su\v{c}\'ik}

\course{Master of Informatics}
\project{
  \vspace{3cm}{\bf MInf Project (Part 2) Report}
}

\date{2020}

\pagenumbering{roman}
\abstract{
  TO-DO
}

\maketitle

\section*{Acknowledgements}{
  I~thank Steve Renals of University of Edinburgh and Vova Vlasov of Rasa for supervising me throughout the~academic year; patiently listening to my never-ending reports and providing helpful and optimistic comments.

  Many thanks also to Ralph Tang whose work inspired this project, and to Sl\'avka He\v{z}elyov\'a who constantly supported me and motivated me to explain all of my work in non-technical stories and metaphors.
}

{
  \hypersetup{linkcolor=black}
  \tableofcontents
}

\chapter{Introduction}{
  \pagenumbering{arabic}
  \setcounter{page}{1}
  % what's the~problem, 
  \section{Motivation}{
    %   \begin{itemize}
    %     \item After the~deep learning hype started, NLP went through an~era of LSTMs. Since 2017, the~area has been becoming dominated by Transformer models pre-trained on large unlabelled corpora.
    Natural language processing (NLP) is concerned with using computational techniques to process and analyse human language: for instance, to automatically compute various grammatical properties of a~sentence or to analyse its meaning.
    Since the~early 2010s, this area has seen significant improvements due to powerful machine learning methods, especially large artificial neural networks.

    In 2017, a~new type of neural model was proposed: the~Transformer \citep{Vaswani_2017}. Since then, numerous Transformer variants were developed \citep{Radford_2018,Devlin_2018,Lan_2019,Liu_2019,Lample_2019} -- many of them improving the~state-of-the-art results on various NLP tasks\footnote{See the~leaderboard of the~popular GLUE benchmark \citep{Wang_2018} at \rurl{gluebenchmark.com/leaderboard}, accessed April 15, 2020.}.
    %     \item As newer and bigger Transformer-based models were proposed in 2018 and 2019, improving on the~SOTA, it was becoming clearer that their big size and low speed was rendering them difficult to use (both train and deploy) in practice outside of research labs.
    However, these successful models are very large (with hundreds of millions of learnable parameters), which makes them computationally expensive and slow. This limits applications of such models outside of research, in scenarios like real-time sentence processing for human-bot conversations\footnote{Take an~automated customer support system -- a~bot. Each customer message gets processed. If the~processing model is slow, multiple model instances have to be deployed in order to handle a~large number of conversations at once, which in turn requires more resources.}.

    %     \item Recently, we've seen various early attempts at making Transformers -- in particular BERT \citep{Devlin_2018} -- smaller by removing attentional heads \citep{Michel_2019}, quantisation and pruning \citep{Cheong_2019, Sucik_2019}. In terms of actually down-sizing and accelerating the~models, knowledge transfer using teacher-student knowledge distillation has led to the~most attractive results \citep{Mukherjee_2019,Tang_2019a,Jiao_2019,Sanh_2019}.
    In an~effort to address this downside, a~recent stream of research has focused on making Transformers -- especially the~widely used BERT model \citep{Devlin_2018} -- smaller and faster \citep{Michel_2019,Cheong_2019}. This includes my own work \citep{Sucik_2019}.
    Primarily, variations on the~\textit{teacher-student knowledge distillation} approach \citep{Bucila_2006} have been used to successfully compress BERT, see \citet{Sun_2019a,Mukherjee_2019,Tang_2019a,Tang_2019b,Jiao_2019,Sanh_2019}.
    In knowledge distillation, a~large, trained model is used as a~\textit{teacher}, and a~smaller \textit{student} model learns by observing and trying to mimic the~teacher's prediction patterns.
    
    %     \item However, these studies focus only on using knowledge distillation as a~tool. Important questions about the~nature of this technique and how it interacts with properties of the~teacher and student models remain generally unexplored.
    
    Using knowledge distillation, BERT can be made several times smaller without significant loss of accuracy.
    While numerous variants of this technique have been successfully developed, there is little understanding of the~nature of knowledge distillation: 
    How and what kinds of the~large model's knowledge are best learned by the~student, and how this depends on the~architecture of the~teacher and student models.
    This gap in understanding is in contrast with the~lot of research in understanding the~internal properties and linguistic capabilities of BERT \citep{Ganesh_2019,Tenney_2019b,Kovaleva_2019,Lin_2019,Rogers_2020}.
    I~argue that it is also important to have a~good understanding of knowledge distillation as a~tool, and of the~smaller and faster models eventually produced by applying this tool to BERT.
        
    %     \item In line with the~increasing demand for explainable AI (TOO BOLD, TALK ABOUT INTERPRETABILITY), it is desirable that, for the~beginning, at least the~researchers better understand the~tools they use, in this case distillation of NLP knowledge from Transformer models. Indeed, such understanding is also useful for overcoming the~limitations and designing new variants of this method for smaller and better classifiers.
    %   \end{itemize}
  }
  
  \section{Aims and contributions}{
    In this work, I~try to better understand knowledge distillation by exploring its use for knowledge transfer from BERT into architecturally diverse students, on various NLP tasks.

    This is further broken down into three aims:
    \begin{itemize}
      \item{
        \textit{Explore the~effectiveness of knowledge distillation for very different NLP tasks.}
        The~chosen tasks focus on identifying the~sentiment, intent, and linguistic acceptability of single sentences.
        
        I~show that the~specific knowledge distillation approach of \citet{Tang_2019b} can be used to distil BERT into extremely small students -- several thousand times smaller and faster -- on two of the~NLP tasks.
        By characterising each task in terms of the~linguistic capabilities it requires, I~explain the~students' inability to match their teacher on the~linguistic acceptability task.
        }
      \item{
        \textit{Explore how distilling knowledge from BERT varies when using different student architectures.}
        In particular, I~use a~down-scaled BERT student architecturally similar to the~teacher, and a~BiLSTM student used previously by \citet{Tang_2019a,Tang_2019b}, very different from the~teacher.

        Both student models are shown to behave similarly.
        As a~novel way of initialising the~student models, I~use trained sub-word embeddings extracted from the~teacher model, and compare them to widely used word embeddings.
        }
      \item{
        \textit{Explore the~linguistic knowledge present in the~teacher and how successfully it is learned by the~students.}
        A~previously proposed \textit{probing} approach \citep{Conneau_2018} is used for measuring and localising diverse linguistic skills within the~models.
        Secondly, I~use a~mostly qualitative approach to mine insights from the~models' decisions and from the~confidence with which the~decisions are made.
        
        I~observe that the~extent to which the~teacher and student models behave similarly depends on the~task.        
        Further, for each task, I~describe examples which are easy or difficult for the~models to classify, and conclude that, in general, the~most sophisticated semantic skills are not learnt well by the~students.
        Finally, I~show that a~model can contain residual language knowledge not needed for the~NLP task, and I~demonstrate how model probing can help explain the~source of such knowledge.
      }
    \end{itemize}
  }
}

\chapter{Background}{
  \label{ch:background}

  In this chapter, the~Transformer models are introduced and set into the~historical context; knowledge distillation is introduced, in particular its recent applications in NLP; and an~overview of the~most relevant work in model understanding is given.

  % NLP (sentence classification), transformers, knowledge distillation, model understanding (probing)
  \section{NLP before Transformers}{
    \label{sec:pre-transformer-nlp}
    % NLP is all about sequences of variable lengths: sentences, sentence pairs, documents, speech segments...
    By the~very nature of natural language, its processing has always meant processing sequences of variable length: be it written phrases or sentences, words (sequences of characters), spoken utterances, sentence pairs, or entire documents.
    % NLP tasks are typically about making simple predictions about sequences: classifying sentences based on their intent or language, scoring a~document's level of formality, predicting whether two sentences form a~coherent question-answer pair or not, predicting the~next word of a~sentence...
    Very often, NLP tasks boil down to making simple decisions about such sequences: classifying sentences based on their intent or language, assigning a~score to a~document based on its formality, deciding whether two given sentences form a~meaningful question-answer pair, or predicting the~next word of an~unfinished sentence.

    % Word vectors used already by \citet{Collobert_Weston_2008,Collobert_Weston_2011}, trained in unsupervised fashion. Used for various NLP tasks in a~CNN.
    As early as 2008, artificial neural networks started playing a~key role in NLP: \citet{Collobert_Weston_2008}\footnote{See also \citet{Collobert_Weston_2011}.} successfully trained a~deep neural model to perform a~variety of tasks from part-of-speech tagging to semantic role labelling.
    % Machine learning predictors are typically designed to work with fixed-size representations of inputs. Therefore, ever since the~resurgence of neural networks around 2010, neural NLP has been using various models for encoding variable-length sequences into common fixed-dimensional representations.
    However, neural machine learning models are typically suited for tasks where the~dimensionality of inputs is known and fixed. Thus, it comes as no surprise that NLP research has focused on developing better models that encode variable-length sequences into fixed-length representations. 
    If any sequence (e.g. a~sentence) can be embedded as a~vector in a~fixed-dimensionality space, a~simple classification model can be learned on top of these vectors.
    
    One key step in the~development of neural sequence encoder models has been the~idea of \textit{word embeddings}: rich, dense, fixed-length numerical representations of words. When viewed as a~lookup table -- one vector per each supported word -- such embeddings can be used to ``translate'' input words into vectors which are then processed further.
    % Word2vec \citet{Mikolov_2013} made embedding learning much more efficient (and made learning the~embedings the~main aim!), using CBOW and skip-grams.
    \citet{Mikolov_2013} introduced an~efficient and improved way of learning high-quality word embeddings: \textit{word2vec}. The~embeddings are learnt as part of the~parameters of a~larger neural network. The~network is forced to learn two tasks: 1) given an~incomplete sentence, predicting its next word, and 2) given a~word from a~sentence, predicting the~words preceding the~given one in the~same sentence\footnote{These are the~so-called Continuous bag-of-words (CBOW) and Skip-gram (SG) tasks, respectively.}. Such training can easily leverage large amounts of unlabelled text data and the~embeddings learn to capture various properties from a~word's morphology to its semantics. The~released word2vec embeddings became very popular due to their easy use and good performance (influential work using word2vec includes \citet{Lample_2016,Kiros_2015,Dos_2014,Kusner_2015}).

    % RNN- and later LSTM-based encoder architectures were dominating the~area for a~long time as they were naturally suited for processing sequences of any length.
    While word embeddings were a~breakthrough, they themselves do not address the~issue of encoding a~sequence of words into a~fixed-size representation. This is where Recurrent neural networks (RNNs) \citep{Rumelhart_1986} come into play.
    Recurrent models process one word at a~time (see \autoref{fig:RNN}) while updating an~internal (``hidden'') fixed-size representation of the~text seen so far.
    Once the~entire sequence is processed, the~hidden representation (also called ``hidden state'') can be output and used to make a~simple prediction.
    \begin{figure}[h!t]
      \centering
      \includegraphics[width=6cm]{graphics/rnn}
      \caption{A recurrent neural network (RNN) consumes at each timestep one input word. Then, it produces a~single vector representation of the~inputs.}
      \label{fig:RNN}
    \end{figure}

    A~common downside of RNNs is that they ``forget'' over longer sequences. This issue is addressed by introducing learnable \textit{gates}, an~idea which soon led to a~recurrent model called the~Long Short-Term Memory network (LSTM) \citep{Hochreiter_Schmidhuber_1997}. An~LSTM unit has a~memory cell and learns to selectively add parts of the~input into the~memory, forget parts of the~memory, and output parts of it (see \autoref{fig:rnn-lstm}). Long after being proposed in 1997, LSTMs gained popularity in NLP -- especially in text processing (see e.g. \citet{Mikolov_2010} and \citet{Graves_2013}). 
    \begin{figure}[h!t]
      \centering
      \includegraphics[width=13cm]{graphics/rnn-lstm}
      \caption{Comparing the~internals of a~vanilla RNN and an~LSTM. The~latter has three gates (shown as $\bigotimes$) -- the~forget gate $F$, the~update gate $U$, and the~output gate $O$. $\bm{c}$ is the~memory cell, $\bm{h}$ is the~internal (hidden) state which can be used as the~output at any timestep. With $\CIRCLE$ is shown a~learnable non-linear transformation.}
      \label{fig:rnn-lstm}
    \end{figure}

    % A~major breakthrough came when \citet{Kalchbrenner_2013} and \citet{Sutskever_2014} developed the~encoder-decoder architecture for machine translation and other sequence-to-sequence tasks such as paraphrasing or parsing.
    As various recurrent models started dominating NLP, one particularly influential architecture emerged, addressing tasks such as machine translation, where the~output is a~new sequence rather than a~simple decision. This was the~\textit{encoder-decoder} architecture (first described by \citet{Hinton_1994}, later re-introduced in the~NLP context by \citet{Kalchbrenner_2013} and \citet{Sutskever_2014}), see \autoref{fig:encoder-decoder}. It uses a~recurrent encoder to turn an~input sentence into a~single vector, and a~recurrent decoder to generate an~output sequence based on the~vector.
    \begin{figure}[h!t]
      \centering
      \includegraphics[width=11.5cm]{graphics/encoder-decoder}
      \cprotect\caption{An encoder-decoder model for machine translation. Notice how the~decoder initially takes as input the~special \verb|<start>| token and at later time consumes the~previous output word.}
      \label{fig:encoder-decoder}
    \end{figure}

    % \citet{Bahdanau_2014} improved things by introducing attention, enabling the~recurrent encoders to learn to selectively attend or ignore parts of the~input sequence.
    \citet{Bahdanau_2014} improved encoder-decoder models by introducing the~concept of \textit{attention}. The~attention module helps the~decoder produce better output by selectively focusing on the~most relevant encoder hidden states at each decoder timestep. This is depicted in \autoref{fig:encoder-decoder-att}, showing the~decoder just about to output the~second word (``est\'as''). The~steps (as numbered in the~diagram) are:
    \begin{enumerate}
      \item the~decoder's hidden state passed to the~attention module,
      \item the~intermediate hidden states of the~encoder also passed to the~attention module,
      \item the~attention module, based on information from the~decoder's state, selecting relevant information from the~encoder's hidden states and combining it into the~attentional \textit{context vector},
      \item the~decoder combining the~last output word (``c\'omo'') with the~context vector and consuming this information to better decide which word to output next. 
    \end{enumerate}

    \begin{figure}[h!t]
      \centering
      \includegraphics[width=11.5cm]{graphics/encoder-decoder-att}
      \cprotect\caption{An encoder-decoder model for machine translation with added attention mechanism.}
      \label{fig:encoder-decoder-att}
    \end{figure}
    
    The~attention can be described more formally\footnote{My description does not exactly follow the~original works of \citet{Bahdanau_2014} and \citet{Luong_2015}. Instead, I~introduce concepts that will be useful in later sections of this work.}: First, the~decoder state $\bm{h}_D$ is processed into a~\textit{query} $\bm{q}$ using a~learnable weight matrix $W_Q$:
    \begin{equation}
    \bm{q}=\bm{h}_DW_Q
    \end{equation}
    and each encoder state $\bm{h}_E^{(i)}$ ($i$ being the~input position or encoder timestep) is used to produce the~\textit{key} and \textit{value} vectors, $\bm{k}^{(i)}$ and $\bm{v}^{(i)}$:
    \begin{equation}
    \bm{k}^{(i)} = \bm{h}_E^{(i)}W_K,\ \ \ \bm{v}^{(i)} = \bm{h}_E^{(i)}W_V\ .
    \end{equation}
    Then, the~selective focus of the~attention is computed as an~\textit{attention weight} $w^{(i)}$ for each input position \textit{i}, by combining the~query with the~$i$-th key:
    \begin{equation}
    w^{(i)}=\bm{q}^\top\bm{k}^{(i)}\ .
    \end{equation}
    The~weights are normalised using softmax and used to create the~context vector $\bm{c}$ as a~weighted average of the~values:
    \begin{equation}
    \bm{c}=\sum_{i}a^{(i)}\bm{v}^{(i)}\ \ \ \textrm{where}\ \ \ a^{(i)}=\softmax(w^{(i)})=\frac{\exp{(w^{(i)})}}{\sum_{j}\exp{(w^{(j)})}}\ .
    \end{equation}
    Note that $W_Q$, $W_K$, $W_V$ are matrices of learnable parameters, optimised in training the~model. This way, the~attention's ``informed selectivity'' improves over time.
    
    For about 4 years, recurrent models with attention were the~state of the~art in many NLP tasks. However, as we will see, the~potential of attention reached far beyond recurrent models.
  }

  \section{Transformer-based NLP}{
    \label{sec:Transformer-based-NLP}
    \subsection{Transformers}{
      \label{sec:Transformers}
      % \citet{Vaswani_2017} introduced Transformer. Main idea: process tokens in parallel, not sequentially, with sequentiality represented by positional markers (embeddings). Self-attention is used to pool from the~context of the~entire sequence, leading to evolving rich contextualised representations of each token in the~higher layers.
      We saw how the~attention mechanism can selectively focus on parts of a~sequence to extract relevant information from it. This raises the~question of whether processing the~inputs in a~sequential fashion with the~recurrent encoder is still needed. In particular, RNN models are slow as a~result of this sequentiality, and are hard to parallelise. In their influential work, \citet{Vaswani_2017} proposed an~encoder-decoder model based solely on attention and fully parallelised: the~\textit{Transformer}. The~core element of the~model is the~\textit{self-attention} mechanism, used to process all input words in parallel.

      In particular, a~Transformer model typically has multiple self-attention layers, each layer processing separate representations of all input words. Continuing with the~three-word input example from \autoref{fig:encoder-decoder-att}, a~high-level diagram of the~workings of a~self-attention layer is shown in \autoref{fig:self-att-layer}. Importantly, the~input word representations evolve from lower to higher layers such that they consider not just the~one input word, but also all other words -- the~representation becomes \textit{contextual} (also referred to as a~\textit{contextual embedding} of the~word within the~input sentence).

      \begin{figure}[h!t]
        \centering
        \includegraphics[width=8cm]{graphics/self-att-layer}
        \cprotect\caption{A high-level diagram of the~application of self-attention in Transformer models. Three hidden states are shown for consistency with the~length of the~input shown in \autoref{fig:encoder-decoder-att}; in general, the~input length can vary.}
        \label{fig:self-att-layer}
      \end{figure}

      As for the~internals of self-attention, the~basic principle is very similar to standard attention. Self-attention too is used to focus on and gather relevant information from a~sequence of elements, given a~query. However, to produce a~richer contextual embedding $\bm{h}_{l+1}^{(i)}$ of the~$i$-th input word in layer $l+1$, self-attention uses the~incoming representation $\bm{h}_l^{(i)}$ for the~query, and considers focusing on all representations in layer $l$, including $\bm{h}_l^{(i)}$ itself. \autoref{fig:self-att} shows this in detail for input position $i=1$. Query $\bm{q}^{(1)}$ is produced and matched with every key in layer $l$ (i.e. $\bm{k}^{(0)},\ldots,\ \bm{k}^{(2)}$) to produce the~attention weights. These weights quantify how relevant each representation $\bm{h}_l^{(i)}$ is with respect to position $i=1$. Then, the~new contextual embedding $\bm{h}_{l+1}^{(i)}$ is constructed as a~weighted sum of the~values $\bm{v}^{(0)},\ldots,\ \bm{v}^{(2)}$ (same as constructing the~context vector in standard attention).

      \begin{figure}[h!t]
        \centering
        \includegraphics[width=12cm]{graphics/self-att}
        \cprotect\caption{The~internals of self-attention, illustrated on creating the~next-layer hidden representation of the~input position $i=1$, given all representations in the~current layer (previous, current, and following). Note that $\otimes$ stands for multiplication (where the~multiplication involves a~learnable matrix like $W_K$, this is written next to the~$\otimes$), and $\oplus$ denotes summation.}
        \label{fig:self-att}
      \end{figure}

      % parallel nature
      Notice that, even though each contextual embedding considers all input positions, the~next-layer contextual embeddings $\bm{h}_{l+1}^{(0)},\ldots,\ \bm{h}_{l+1}^{(2)}$ can be computed all at the~same time, in parallel: First, the~keys, queries and values for all input positions are computed; then, the~attention weights with respect to each position are produced; finally, all the~new representations are produced. It is this parallelism that allows Transformer models to run faster. As a~result, they can be much bigger (and hence create richer input representations) than recurrent models while taking the~same time to train.

      % positional embeddings
      Due to their parallel nature, self-attentional layers have no notion of an~element's position within the~input sequence. This means no sensitivity to word order. (Recurrent models sense this order quite naturally because they process input text word by word.) To alleviate this downside of self-attention, Transformers use \textit{positional embeddings}. These are artificially created numerical vectors added to each input word, different across input positions, thus enabling the~model's layers to learn to be position- and order-sensitive.

      % multi-headedness
      As an~additional improvement of the~self-attentional mechanism, \citeauthor{Vaswani_2017} introduce the~concept of multiple self-attention heads. This is very similar to having multiple instances of the~self-attention module in \autoref{fig:self-att}, each instance being one \textit{head} and computing its own queries, keys and values. The~motivation behind multiple self-attention heads is to enable each head $a$ to learn different ``focusing skills'' by learning its own $W_{Q,a}$, $W_{K,a}$, $W_{V,a}$. Each head produces its own output\footnote{Here, $\bm{q}$, $\bm{k}$, $\bm{v}$, $\bm{h}$ are column vectors.}:
      \begin{equation}
      \bm{O}_{att,a} = \softmax(\frac{\bm{q}\bm{k}}{\sqrt{d_k}})\bm{v} = \softmax(\frac{(\bm{h}_l^\top{W_{Q,a}})(\bm{h}_l^\top{W_{K,a}})}{\sqrt{d_k}})(\bm{h}_l^\top{W_{V,a}})
      \end{equation}
      which matches \autoref{fig:self-att} (but notice the~detail of the~additional scaling by $\frac{1}{\sqrt{d_k}}$, introduced by \citeauthor{Vaswani_2017}, where $d_k$ is the~dimensionality of the~key).
      The~outputs of the~$A$ individual attentional heads are then concatenated and dimensionality-reduced with a~trainable linear transformation $W_{AO}$, to produce the~final output, which replaces $\bm{h}_{l+1}$ in \autoref{fig:self-att}:
      \begin{equation}
      \bm{O}_{att} = [\bm{O}_{att, 1},\ \ldots,\ \bm{O}_{att, A}]W_{AO}\ .
      \end{equation}
      
      % \citet{Radford_2018} introduced the~idea of generative LM pre-training and fine-tuning. This concept helps train much better models even for low-resource tasks with small datasets, by leveraging general language knowledge acquired by the~model in the~pre-training phase. Publishing pre-trained model instances makes the~power of NLP much more accessible to anyone and has become a~popular thing to do. (Also mention that subword tokens were used instead of words.)
      Besides the~self-attention-based architecture, there is one more important property that makes today's Transformer models perform so well on a~wide variety of NLP tasks: the~way these models are trained. First used for Transformers by \citet{Radford_2018}\footnote{The~idea was previously used with recurrent models by \citet{Dai_2015}.}, the~general procedure is:
      \begin{enumerate}
        \item \textit{Unsupervised pre-training}: The~model is trained on one or more tasks, typically language modelling, using huge training corpora. For example, \citeauthor{Radford_2018} pre-train their model to do next word prediction (the~standard language modelling task) on a~huge corpus of over 7,000 books.
        \item \textit{Supervised fine-tuning}: The~pre-trained model is trained on a~concrete dataset to perform a~desired downstream task, such as predicting the~sentiment of a~sentence, translating between languages, etc.
      \end{enumerate}
      This two-step procedure is conceptually similar to using pre-trained word embeddings. In both cases, the~aim is to learn general language knowledge and then use this as a~starting point for focusing on a~particular task. However, in this newer case, the~word representations learned in pre-training are better tailored to the~specific architecture, and they are inherently contextual -- compared to pre-trained word embeddings like word2vec which are typically context-insensitive. 

      Importantly, pre-trained knowledge makes models more suitable for downstream tasks with limited amounts of labelled data. The~model no longer needs to acquire all the~desired knowledge just from the~small dataset; it contains pre-trained high-quality general language knowledge which can be reused in various downstream tasks. This means that large, powerful Transformer models become more accessible: They are successfully applicable to a~wider array of smaller tasks than large models that have to be trained from scratch.
    }

    \subsection{BERT}{
      \label{sec:BERT}
      % \citet{Devlin_2018} improved the~concept by pre-training the~model bidirectionally (leading to language modelling based on left \textit{and} right context).
      Perhaps the~most popular Transformer model today is BERT (Bidirectional Encoder Representations from Transformers), proposed by \citet{Devlin_2018}. Architecturally, it is a~sequence encoder, hence suited for sequence classification tasks. While being heavily based on the~original Transformer \citep{Vaswani_2017}, BERT also utilises a~number of further ideas:
      \begin{enumerate}
        \item The~model learns bidirectional representations: It can be trained on language modelling that is not next-word prediction (prediction given left context), but word prediction given both the~left and the~right context.
        \item {It uses two very different pre-training classification tasks:
        \begin{enumerate}
          \item The~\textit{masked language modelling} (MLM) task encourages BERT to learn good contextual word embeddings. The~task itself is to correctly predict the~token at a~given position in a~sentence, given that the~model can see the~entire sentence with the~target token(s) masked out\footnote{I.e. replaced with the~special \verb|[MASK]| token.}, replaced with a~different token, or left unchanged.
          
          \item The~\textit{next-sentence prediction} (NSP) task encourages BERT to learn good sentence-level representations. Given two sentences, the~task is to predict whether they formed a~consecutive sentence pair in the~text they came from, or not.
        \end{enumerate}
        The~pre-training was carried out on text from books and from the~English Wikipedia, totalling to 3,400 million words (for details see \citet{Devlin_2018}). The~MLM and NSP tasks were both used throughout the~pre-training, forcing the~model to learn both at the~same time.
        }
        \item The~inputs are processed not word by word, but are broken down using a~fixed vocabulary of sub-word units called \textit{wordpieces} (conceptually introduced by \citet{Sennrich_2016}, this particular variant created by \citet{Wu_2016}). This way, BERT can better deal with rare words -- by assembling them from pieces\footnote{In word-level models, words that are not found in the~model's vocabulary are replaced with a~special \verb|UNKNOWN| token, which means disregarding any information carried by the~words.}. The~tokeniser module of BERT uses the~wordpiece vocabulary of \citeauthor{Wu_2016} to tokenise (segment) the~input text before it is further processed. \autoref{fig:bert-inputs} shows an~example; notice how my surname (``Sucik'') gets split into three wordpieces whereas the~other, much more common words are found in the~wordpiece vocabulary.
        \item To enable the~different pre-training tasks as well as two-sentence inputs, BERT uses a~special input sequence format, illustrated in \autoref{fig:bert-inputs}. Given the~two input sentences $S_A$, $S_B$, they are concatenated and separated by the~special \verb|[SEP]| token. The~overall sequence is prepended with the~\verb|[CLS]| (classification) token. To explicitly capture that certain tokens belong to $S_A$ and others to $S_B$, simple \textit{token type embeddings} (which only take on two different values) are added to the~token embedding at each position. Then, for tasks like NSP, only the~output representation of the~\verb|[CLS]| token (i.e. $\bm{o_0}$) is used, whereas for token-level tasks like MLM the~output vector from the~desired position is used (in \autoref{fig:bert-inputs}, the~MLM task would use $\bm{o_3}$ to predict the~correct token at this position).
      \end{enumerate}

      \begin{figure}[h!t]
        \centering
        \includegraphics[width=14cm]{graphics/bert-inputs}
        \caption{BERT's handling of input for sentence-level and token-level tasks. The~input sentences ($S_A = How\ are\ you,\ Sam\ Sucik?$ and $S_B = Good,\ you?$) are shown as split by BERT's tokeniser, with the~first instance of ``you'' masked out for MLM.}
        \label{fig:bert-inputs}
      \end{figure}
      % They also changed the~pre-training to 2 tasks trained at the~same time: masked language modelling to learn to understand words, and next sentence prediction to learn to reason about entire sentences (as the~actual NLP tasks often require such reasoning). 
      % This is is how BERT was born, which then became extremely popular in the~community, attracting a~lot of work on improving it, analysing its capabilities, extending it to other languages, and even applying it to multi-modal tasks such as video captioning. 
      The~overall architecture of BERT is shown in \autoref{fig:bert-hl}. The~tokeniser also adds the~special tokens like \verb|[CLS]| and \verb|[SEP]| to the~input, while the~trainable token embedding layer also adds the~positional embedding and the~token type embedding to the~wordpiece embedding of each individual token. The~pooler takes the~appropriate model output (for sequence level classification the~first output $\bm{o_0}$ as discussed above) and applies a~fully-connected layer with the~tanh activation function.
      The~external classifier is often another fully-connected layer with the~tanh activation, producing the~logits\footnote{For a~classifier, the~logits are the~(unnormalised) predicted class probabilities.}. These get normalised using softmax to produce a~probability distribution over all classes. The~most probable class gets output as the~model's prediction.
      
      \begin{figure}[h!t]
        \centering
        \includegraphics[width=6cm]{graphics/bert-hl}
        \caption{High-level overview of the~modules that make up the~architecture of BERT as used for sequence-level classification.}
        \label{fig:bert-hl}
      \end{figure}

      % things beyond what was said about Transformers already: residual connections, GeLU?
      To complete the~picture of BERT, \autoref{fig:bert-encoder-layer} shows the~internals of an~encoder layer. Besides the~multi-headed self-attention submodule, it also contains the~fully-connected submodule. This uses a~very wide intermediate fully-connected transformation with parameters $W_I$, inflating the~representations up to the~dimensionality $d_I$, and the~layer output fully-connected transformation with parameters $W_O$, which reduces the~dimensionality. Each submodule is also by-passed by a~residual connection (shown with dashed lines). The~residual information is summed with the~submodule's output, and layer normalisation is applied to the~sum. Note that this structure is not new in BERT; it was used already by the~original Transformer of \citet{Vaswani_2017}. Conveniently, Transformers are designed such that all of the~intermediate representations (especially the~encoder inputs and outputs, and the~self-attention layer inputs and outputs) have the~same dimensionality $d_h$ -- this makes any residual by-passing and summing easy.

      \begin{figure}[h!t]
        \centering
        \includegraphics[width=6cm]{graphics/bert-encoder-layer}
        \caption{The~modules making up one encoder layer in BERT; residual connections highlighted by using dashed lines. $\bigotimes$ marks learnable neural layers, $\bigoplus$ marks summation (in this case used to combine residual information with layer outputs).}
        \label{fig:bert-encoder-layer}
      \end{figure}

      When training BERT, artificial, intentional corruption of internal representations is done using dropout, which acts as a~regulariser, making the~training more robust. In particular, dropout is applied to the~outputs of the~embedding layer, to the~computed attention weights, just before residual summation both to the~self-attention layer output and to the~fully connected layer output (see \autoref{fig:bert-encoder-layer} for the~summation points), and to the~output of the~pooler module (before applying the~external classifier, see \autoref{fig:bert-hl}). The~typical dropout rate used is 0.1.
      
      For updating the~learnable parameters during training, BERT uses the~popular Adam learning algorithm \citep{Kingma_2014}, which combines two main ideas:
      \begin{enumerate}
        \item \textit{Adaptive learning rates}, meaning that each learnable model parameter can have its own ``pace of learning''. In Adam, this individual pace is based on the~observed recent gradients of the~overall model error with respect to the~single parameter.
        \item \textit{Momentum}, a~mechanism used to deal with complex, stochastic error surfaces, by preferring only that direction in the~parameter space, which leads to stable improvements (and dispreferring directions which only result in short-term, stochastic improvements). Two decay rates $\beta_1$ and $\beta_2$ realise the~momentum -- they control how quickly and noisily or slowly and smoothly the~adaptation of the~learning rate happens. In practice, the~high values $\beta_1=0.9$ and $\beta_2=0.999$ are often used (as recommended by \citeauthor{Kingma_2014}), meaning relatively slow and smooth adaptation.
      \end{enumerate}
      
      Originally, pre-trained BERT was released in two sizes: BERT\textsubscript{Base} with 110 million parameters, 12 encoder layers and 12-head self-attention, and BERT\textsubscript{Large} with 340 million parameters, 24 encoder layers and 16-head self-attention. The~models quickly became popular, successfully applied to various tasks from document classification \citep{Adhikari_2019} to video captioning \citep{Sun_2019}. Further pre-trained versions were released too, covering, for example, the~specific domain of biomedical text \citep{Lee_2019} or multilingual text \citep{Pires_2019}.
    }

    \subsection{Newer and larger Transformer models}{
      \label{sec:post-BERT-models}
      % Following the~success of BERT, further and often bigger Transformer models started emerging:
      Following the~success of the~early Transformers and BERT \citep{Vaswani_2017,Radford_2018,Devlin_2018}, many further model variants started emerging, including:
      \begin{itemize}
        \item The~OpenAI team releasing GPT-2 \citep{Radford_2019}, a~larger and improved version of their original, simple Transformer model GPT \citep{Radford_2018}.
        \item \citet{Lample_2019} introducing XLM, which uses cross-lingual pre-training and is thus better suited for downstream tasks in different languages.
        \item Transformer-XL \citep{Dai_2019}, which features an~improved self-attention that can handle very long contexts (across multiple sentences/documents).
      \end{itemize}

      All these open-sourced, powerful pre-trained models were a~significant step towards more accessible high-quality NLP (in the~context of downstream tasks with limited data). However, the~model size -- often in 100s of million trainable parameters -- meant these models could not be applied easily in practice (outside of research): They were memory-hungry and slow.
      
      Naturally, this inspired another stream of research: Compressing large, well-performing Transformer models (very often BERT) to make them faster and resource-efficient.
      I~turn my focus to one compression method that worked particularly well so far: the~teacher-student knowledge distillation.
    }
  }

  \section{Teacher-student knowledge distillation}{
    \label{sec:KD}
    % brief history of KD in general
    % different objectives: logits, hard labels, mimicking internal representations...
    \subsection{A brief introduction to knowledge distillation}{
      \label{sec:KD-intro}
      Knowledge distillation was introduced by \citet{Bucila_2006} as a~way of knowledge transfer from large models into small ones. The~aim is to end up with a~smaller -- and hence faster -- yet well-performing model. The~steps are 1) to train a~big neural classifier model (also called the~\textit{teacher}), 2) to let a~smaller neural classifier model (the~\textit{student}) learn from it -- by learning to mimic the~teacher's behaviour. Hence also the~name \textit{teacher-student knowledge distillation}, often simply \textit{knowledge distillation}.
      
      There are different ways of defining the~teacher's ``behaviour'' which the~student learns to mimic. Originally, this was realised as learning to mimic the~teacher's predictions: A~dataset would be labelled by the~teacher, and the~student would be trained on these labels (which are in this context referred to as the~\textit{hard labels}). The~dataset used for training the~student (together with the~teacher-generated labels) is referred to as the~\textit{transfer dataset}.

      Later, \citet{Ba_2013} introduced the~idea of learning from the~teacher-generated \textit{soft labels}, which are the~teacher's logits. The~idea is to provide the~student with richer information about the~teacher's decisions: While hard labels only express which class had the~highest predicted probability, soft labels also describe how confident the~prediction was and which other classes (and to what extent) the~teacher was considering for a~given example.

      When soft labels were first used, the~student's training loss function was the~mean squared distance between the~student's and the~teacher's logits:
      \begin{equation}
        E_{MSE}=\sum_{c=1}^{C}{(z_t^{(c)}-z_s^{(c)})^2}
        \label{eq:E_MSE}        
      \end{equation}
      where $C$ is the~number of classes and $z_t$, $z_s$ are the~teacher's and student's logits.
      \citet{Hinton_2015} proposed a~more general approach, addressing the~issue of overconfident teachers with very sharp logit distributions. The~issue with such distributions is that they carry little additional information beyond the~hard label (since the~winning class has a~huge probability and all others have negligibly small probabilities).
      To ``soften'' such sharp distributions, \citeauthor{Hinton_2015} proposed using the~\textit{cross-entropy loss} \autoref{eq:E_CE} in combination with \textit{softmax with temperature} \autoref{eq:softmax-temperature} (instead of the~standard softmax) in training both the~teacher and the~student.
      \begin{equation}
        E_{CE}=\sum_{c=1}^{C}{z_t^{(c)} \log{z_s^{(c)}}}
        \label{eq:E_CE}        
      \end{equation}
      \begin{equation}
        p_c=\frac{\exp{(z^{(c)}/T})}{\sum_{c'=1}^{C}{\exp{(z^{(c')}/T)}}}
        \label{eq:softmax-temperature}        
      \end{equation}
      The~temperature parameter $T$ determines the~extent to which the~distribution will be ``unsharpened'' -- two extremes being the~completely flat, uniform distribution (for $T \rightarrow \infty$) and the~maximally sharp distribution\footnote{I.e. having the~preferred class's probability 1 and the~other classes' probabilities 0.} (for $T \rightarrow 0$). When $T > 1$, the~distribution gets softened and the~student can extract richer information from it. Today, using soft labels with the~cross-entropy loss with temperature is what many refer to simply as knowledge distillation.

      Since 2015, further knowledge distillation variants have been proposed, enhancing the~vanilla technique in various ways, for example:
      \begin{itemize}
        \item \citet[p. 13]{Papamakarios_2015} points out that mimicking teacher outputs can be extended to mimicking the~\textit{derivatives} of the~teacher's loss with respect to the~inputs. This is realised by including in the~student's loss function also the~term: $\frac{\partial \bm{o}_s}{\partial \bm{x}} - \frac{\partial \bm{o}_t}{\partial \bm{x}}$ ($\bm{x}$ being an~input, e.g. a~sentence, and $\bm{o}$ being the~output, e.g. the~predicted class). %, the~additional loss term being calculated using the~R technique (Pearlmutter, 1994).
        \item \citet{Romero_2015} proposed to additionally match the~teacher's internal, intermediate representations of the~input. \citet{Huang_2017} achieved this by learning to align the~distributions of neuron selectivity patterns between the~teacher's and the~student's hidden layers. Unlike standard knowledge distillation, this approach is no longer limited only to classifier models with softmax outputs (see the~approach of \citet{Hinton_2015} discussed above).
        \item \citet{Sau_2016} showed that learning can be more effective when noise is added to the~teacher logits.
        \item \citet{Mirzadeh_2019} showed that when the~teacher is much larger than the~student, knowledge distillation performs poorly, and improved on this by ``multi-stage'' distillation: First, knowledge is distilled from the~teacher into an~intermediate-size ``teacher assistant'' model, then from the~assistant into the~final student.
      \end{itemize}
    }

    \subsection{Knowledge distillation in NLP}{
      \label{sec:kd-nlp}
      % \citet{Kim_2016} observe that KD and weight pruning are orthogonal (can be used together), and that mimicking top-most hidden layer outputs (instead of outputs themselves) doesn't provide improvements previously reported.
      % Practically all the~so far mentioned research in knowledge distillation was done in the~domain of image processing. This comes as no surprise: It was image processing that was benefitting the~most from the~resurgence of deep learning. Ever since the~AlexNet \citep{Krizhevsky_2012}, bigger and bigger models were proposed, simultaneously driving the~research in model compression so as to make the~models usable in practice.
      The~knowledge distillation research discussed so far was tied to the~image processing domain. This is not surprising: Image processing was the~first area to start taking advantage of deep learning, and bigger and bigger models had been researched ever since the~revolutional AlexNet \citep{Krizhevsky_2012}.

      % KD in NLP: sequence-level KD \citep{Kim_2016}, then basically straight to distilling from BERT?
        % In the~NLP literature, it has previously been used in neural machine translation (Kim and Rush, 2016) and language modeling (Yu et al., 2018).
      % In natural language processing, research on knowledge distillation was rare for a~long time. One notable work was the~adaptation of distillation for sequence-to-sequence machine translation models -- whose outputs are no longer simple classification scores -- by \citet{Kim_2016}. Another pioneering study compressed a~recurrent neural language model for use on mobile devices \citep{Yu_2018}.
      In NLP and in text processing in particular, the~(recurrent) models were moderately sized for a~long time, not attracting much research in model compression. Still, one early notable work was on adapting knowledge distillation for sequence-to-sequence models \citep{Kim_2016}, while another pioneering study \citep{Yu_2018} distilled a~recurrent model into an~even smaller one -- to make it suitable for running on mobile devices.

      Understandably, the~real need for model compression started very recently, when the~large pre-trained Transformer models became popular. Large size and low speed seemed to be the~only downside of these -- otherwise very successful and accessible -- models.

      % When distilling knowledge from big, pre-trained Transformer models, the~main decision is whether to distil before or after fine-tuning on a~concrete downstream task. Each option has its pros and cons.
      Perhaps the~first decision to make when distilling large pre-trained models is at which point to distil. In particular, one can distil the~general knowledge from a~pre-trained teacher and use such a~general student by fine-tuning it on downstream tasks, or one can fine-tune the~pre-trained teacher on a~task and then distil this specialised knowledge into a~student model meant for the~one task only. Each of these approaches has its advantages and disadvantages.

      In the~first scenario (distilling pre-trained knowledge), a~major advantage is that the~distillation happens once and the~small student can be fine-tuned quickly for various downstream tasks.
      Since the~distillation can be done on the~same data that the~teacher was pre-trained on -- large unlabelled text corpora --, lack of transfer data is not a~concern.
      A~possible risk is that the~large amount of general pre-trained language knowledge will not ``fit'' into the~small student, requiring the~student itself to be relatively large. \citet{Sanh_2019} took this approach and, while their student can be successfully fine-tuned for a~wide range of tasks, it is only 40\% smaller than the~BERT\textsubscript{Base} teacher.

      % Distilling beforeinto a~small student and subsequently finetuning it on any downstream task means easy 
      %   pros: have flexible model that's easy to finetune for any downstream task. lots of transfer data to work with.
      %   cons: too small model may not be able to take all of teacher's useful knowledge
      %   Sanh tried and only got to making BERT 60\% smaller??? (# of params).

      % CAREFUL NOT TO MIX IN OWN METHODOLOGY HERE!!!
      In the~second scenario, only the~task-specific knowledge needs to be transferred to the~student -- potentially allowing smaller students.
      However, teacher fine-tuning and distillation have to be done anew for each task and this is resource-hungry.
      Additionally, there may be a~lack of transfer data if the~downstream task dataset is small.
      Various ways of addressing this issue by \textit{augmenting} small datasets have been proposed, with mixed success. 
      \citet{Mukherjee_2019} use additional unlabelled in-domain sentences with labels generated by the~teacher -- this is limited to cases where such in-domain data are available. \citet{Tang_2019a} create additional sentences using simple, rule-based perturbation of existing sentences from the~downstream dataset. Finally, \citet{Jiao_2019} and \citet{Tang_2019b} use large Transformer models generatively to create new sentences. In the~first case, BERT is applied repeatedly to an~existing sentence, changing words into different ones one by one and thus generating a~new sentence. In the~second case, new sentences are sampled token-by-token from a~GPT-2 model fine-tuned on the~downstream dataset with the~next-token-prediction objective.
      % After:
      %   pros: only task-specific knowledge is distilled, likely into much smaller model.
      %   cons: have to train big teacher for each downstream task, likely to take long. may need more transfer data than small downstream dataset.
      %   Tang: distilled into much smaller student. didn't address issue of having to train teacher repeatedly. addressed issue of little data quite successfully.

      % In this work, I~adopt the~approach of \citet{Tang_2019b} as I~view it as the~most promising one so far. However, while they use only bidirectional LSTM students, I~also experiment with a~smaller version of BERT, similarly to \citet{Jiao_2019}. For detailed description of the~system see \autoref{chap:implementation}.
      Clearly, each approach is preferred in a~different situation: If the~requirement is to compress the~model as much as possible, and there is enough transfer data, distilling the~fine-tuned teacher is more promising. If, on the~other hand, one wants to make available a~re-usable, small model, then distilling the~broader, pre-trained knowledge is preferred.
    }
  }
  
  \section{Interpreting NLP models}{
    \label{sec:understanding-models}
    % NNs are black boxes and (not) understanding the~models is a~serious issue.
    Neural models are by their very nature opaque or even black boxes, and not properly understanding them is a~serious concern.
    % performance is typically more important than transparence, but recently the~demand for explainable AI (XAI) has been increasing. additionally, understanding is opportunity for further improvements of the~mdels and techniques.
    Despite the~typical preference of performance over transparency, recently, the~demand for explainable artificial intelligence (XAI) has been increasing, as neural models become widely used. (Besides the~DARPA XAI program\footnote{\rurl{www.darpa.mil/program/explainable-artificial-intelligence}}, conferences like the~International Joint Conference on Artificial Intelligence (IJCAI), the~SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), and the~Conference on Computer Vision and Pattern Recognition (CVPR) now feature dedicated XAI workshops\footnote{See \rurl{sites.google.com/view/xai2019/home}, \rurl{xai.kdd2019.a.intuit.com/}, \rurl{explainai.net/}.}.)

    % in image processing, interpretability is easy thanks to visualising things. (somewhat similarly music.) notable works: maximising activation, visualising neurons' output for given input, maximum activation samples.
    The~area of image processing has seen the~most attempts at interpreting neural models and their behaviour. One reason being that visiual tasks are often doable and easy to reason about for researchers and for humans in general. Various techniques shed light into the~behaviour of image classifiers; for instance, techniques for creating images that maximally excite certain neurons \citep{Simonyan_2013}, or highlighting those parts of an~image that a~particular neuron ``focuses'' on \citep{Zeiler_2013}. 
    
    In NLP, interpretation is more difficult. Additionally, most research in interpreting NLP models started only relatively recently, after large neural models became widely used.
    % hard to do maximising activation or visualising single units. BUT types of information preserved in internal input representations can be well explored: probing!
    In their review, \citet{Belinkov_2018} note that many methods for analysing and interpreting models are simply adapted from image processing, especially the~approach of visualising a~single neuron's focus, given an~input.
    In attentional sequence-to-sequence models, the~attention maps can be visualised to explore the~soft alignments between input and output words (see, e.g., \citet{Strobelt_2018}). However, these methods are mostly qualitative and suitable for exploring individual input examples, thus not well suited for drawing statistically backed conclusions or for quantitative model comparison.
    
    More quantitative and NLP-specific are the~approaches that explore the~linguistic knowledge present in a~model's internal representations.
    Most often, this is realised by \textit{probing} the~representations for specific linguistic knowledge: trying to automatically recover from them specific properties of the~input. When such recovery works well, the~representations must have contained the~linguistic knowledge tied to the~input property in question.
    First used by \citet{Shi_2016} for exploring syntactic knowledge captured by machine translation models, this general approach was quickly adopted more widely.
    \citet{Adi_2017} explored sentence encodings from recurrent models by probing for simple properties like sentence length, word content and word order.
    More recently, \citet{Conneau_2018} curated a~set of 10 probing tasks ranging from easy surface properties (e.g. sentence length) through syntactic (e.g. the~depth of the~syntactic parse tree) to semantic ones (e.g. identifying semantically disrupted sentences).
    Focusing on Transformers, \citet{Tenney_2019a} proposed a~set of \textit{edge probing} tasks, examining how much contextual knowledge about an~entire input sentence is captured within the~contextual representation of one of its words.
    Their tasks correspond to the~typical steps of a~text processing pipeline -- from part-of-speech (POS) tagging to identifying dependencies and entities to semantic role labelling. 
    \citet{Tenney_2019b} managed to localise the~layers of BERT most important for each of these ``skills''. They showed that the~ordering of these ``centres of expertise'' within BERT's encoder matches the~usual low- to high-level order: from simple POS tagging in the~earlier layers to more complex semantic tasks in the~last layers.

    While the~discussed approaches provide valuable insights, they merely help us intuitively describe or quantify the~kinds of internal knowledge/expertise present in the~models. 
    \citet{Gilpin_2018} call this level of model understanding \textit{interpretability} -- comprehending what a~model does. 
    However, they argue that what we should strive to achieve is \textit{explainability}: the~ability to ``summarize the~reasons for neural network behavior, gain the~trust of users, or produce insights about the~causes of their decisions''.
    In this sense, today's methods achieve only interpretability because they enable researchers to describe but not explain -- especially in terms of causality -- the~internals and decisions of the~models.
    Still, interpreting models is an~important step not only towards explaining them, but also towards understanding the~properties of different architectures and methods and improving them.
    
    % In this work, I~also attempt to mainly \textit{interpret} the~student and teacher models. I~adopt two approaches:
    % \begin{enumerate}
    %   \item analysing the~mistakes the~models make on the~downstream task they were trained to do, including how confident the~correct and incorrect predictions are
    %   \item probing the~models using the~probing tasks curated by \citet{Conneau_2018}
    % \end{enumerate}
    % By comparing the~findings between models trained on different downstream tasks or with different architectures, I~try to characterise each task in terms of the~linguistic capabilities it utilises. 
    % Further, I~describe how different student model architectures influence how linguistic knowledge is distilled from a~teacher and stored in the~student, and what the~effect on the~student's confidence is. 
    % Finally, I~try to relate the~observed effects to the~method of knowledge distillation itself.
  }

  \section{Summary}{
    % deep neural models widely used in NLP
    Since around 2013, the~area of NLP has been taking advantage of deep neural models.
    % recently even larger Transformer models -- accessible but slow
    With the~introduction of Transformers, the~models became even deeper and more powerful.
    Today's pre-trained Transformer-based models like BERT make state-of-the-art NLP relatively accessible, but the~models are often too large and slow for practical applications.
    % compression (KD) revival
    Compressing such models has become an~active research area, with knowledge distillation being a~particularly successful compression technique.
    % better understanding of new models as well as techniques desired
    However, the~self-attentional, Transformer-based models, as well as compressing them, are still relatively young concepts.
    More research is needed to better interpret the~behaviour of models like BERT, and to better understand the~nature of the~knowledge transfer from large Transformers into smaller, compressed ones.
  }
}

\chapter{Datasets}{
  \label{chap:datasets}

  In this chapter, I~introduce the~different datasets used throughout the~work:
  \begin{enumerate}
    \item To later experiment with models in the~context of a~wide range of NLP tasks, I~use different small \textbf{downstream task datasets} on which I~train large Transformer models.
    \item For knowledge distillation from the~large into smaller models, large \textbf{transfer datasets} are used, created from the~downstream datasets using data augmentation.
    \item Finally, \textbf{probing datasets} are used for analysing the~linguistic capabilities of the~large and the~small models.
  \end{enumerate}

  \section{Downstream tasks}{
    The~downstream task datasets I~use to fine-tune the~teacher model. The~tasks are chosen to be diverse so that the~knowledge distillation analysis later in this work is set in a~wide NLP context. At the~same time, all the~datasets are rather small and therefore well representing the~type of use case where pre-trained models like BERT are desirable due to the~lack of labelled fine-tuning data.

    Today, perhaps the~most widely used collection of challenging NLP tasks\footnote{Challenging by the~nature of the~tasks and by the~small dataset size.} is the~GLUE benchmarking collection \citep{Wang_2018}.
    This collection comprises 11 tasks which enable model benchmarking on a~wide range of NLP problems from sentiment analysis to detecting textual similarity, all framed as single-sentence or sentence-pair classification.
    Each task comes with an~official scoring metric (such as accuracy or F1), labelled training and evaluation datasets, and a~testing dataset with labels not released publicly.
    The~test-set score accumulated over all 11 tasks forms the~basis for the~popular GLUE leaderboard\footnote{\rurl{gluebenchmark.com/leaderboard}}.
    
    In this work, I~use single-sentence classification tasks (i.e. not sentence-pair tasks). Therefore, only two GLUE tasks are suitable for my purposes -- the~Corpus of Linguistic Acceptability (CoLA) and the~Stanford Sentiment Treebank in its binary classification variant (SST-2). 
    Additionally, I~choose a~third task to make my work cover the~area of conversational language. This way, I~build on my previous research in compressing BERT for conversational tasks \citep{Sucik_2019}, undertaken as part of an~internship with Rasa\footnote{\rurl{rasa.com}}, a~company building open-source tools for conversational AI. The~third dataset, called Sara, focuses on classifying human messages (from human-bot conversations) according to their intent.

    \subsection{Corpus of Linguistic Acceptability}{
      \label{sec:datasets-CoLA}

      The~CoLA dataset \citep{CoLA-paper} comprises roughly 8,500 training sentences, 1,000 evaluation and 1,000 testing sentences.
      The~task is to predict whether a~given sentence represents acceptable English or not (binary classification).
      All the~sentences are collected from linguistic literature where they were originally hand-crafted to demonstrate various linguistic principles and their violations.
      
      The~enormous variety of principles, together with many hand-crafted sentences that comply with or violate a~principle in a~niche way, make this dataset very challenging even for the~state-of-the-art Transformer models. 
      As a~non-native speaker, I~myself struggle with some of the~sentences, for instance:
      \begin{itemize}
        \item \textit{*The~car honked down the~road.} (unacceptable\footnote{The~``*'' is a~standard way to mark ungrammatical sentences in linguistic literature.})
        \item \textit{Us, we'll go together.} (acceptable)
      \end{itemize}

      There are many examples which are easy for humans to classify but may be challenging for models which have imperfect understanding of the~real world. Sentences like ``Mary revealed himself to John.'' require the~model to understand that ``Mary'', being a~typical female name, disagrees with the~masculine ``himself''.
      
      The~scoring metric is Matthew's Correlation Coefficient (MCC) \citep{Matthews_1975}, a~correlation measure between two binary classifications. The~coefficient is also designed to be robust against class imbalance, which is important because the~dataset contains many more acceptable examples than unacceptable ones\footnote{For details on the class imbalance, see \autoref{fig:class-balance} in \autoref{chap:A-datasets}.}.
    }

    \subsection{Stanford Sentiment Treebank}{
      \label{sec:datasets-SST-2}

      The~SST-2 dataset \citep{SST-paper} is considerably bigger than CoLA, with roughly 67,000 training examples, 900 evaluation and 1,800 testing examples. It contains sentences and phrases from movie reviews collected on \rurl{rottentomatoes.com}. The~main SST dataset comes with human-created sentiment annotations on the~continuous scale from very negative to very positive. SST-2 is a~simplified version with neutral-sentiment phrases removed, only containing binary sentiment labels (positive and negative).

      Unlike the~hand-crafted examples in CoLA, many examples in SST-2 are not the~best-quality examples. In particular, sentences are sometimes split into somewhat arbitrary segments\footnote{This is due to the~use of an~automated parser in creating the~dataset.}, such as:
      \begin{itemize}
        \item \textit{should have been someone else - } (negative)
        \item \textit{but it could have been worse.} (negative)
      \end{itemize}
      
      The~labels are also sometimes unclear, see:
      \begin{itemize}
        \item \textit{american chai encourages rueful laughter at stereotypes only an~indian-american would recognize.} (negative)
        \item \textit{you won't like roger, but you will quickly recognize him.} (negative)
      \end{itemize}

      Despite the~problematic examples, most are straightforward (e.g. ``delightfully cheeky'' or ``with little logic or continuity''), making this task a~relatively easy one. With accuracy being the~official metric, best models in the~GLUE leaderboard score over 97\%, very close to the~official human baseline of 97.8\%\footnote{See the~GLUE leaderboard at \rurl{gluebenchmark.com/leaderboard}}.
    }

    \subsection{Sara}{
      \label{sec:datasets-Sara}

      As the~third task, I~use an~intent classification dataset created by Rasa, a~start-up building open-source tools for conversational AI\footnote{For transparency: My co-supervisor for this work -- Vladimir Vlasov -- is a~Rasa employee, and he also supervised me during my Machine learning research internship with Rasa in the~summer of 2019.}.

      The~dataset is named Sara after the~chatbot deployed on the~company's website\footnote{See the~bot in action at \rurl{rasa.com/docs/getting-started/}.}.
      The~Sara chatbot is aimed for holding conversations with the~website visitors on various topics, primarily answering common questions about Rasa and the~tools that it develops (the~same tools were used to build Sara). Simultaneously, the Sara dataset is used for most of research at Rasa.
      To support diverse topics, Sara internally classifies each human message as one of 57 intents and then generates an~appropriate response. The~Sara dataset is a~collection of human-generated message examples, each manually labelled with one of the~57 intents, e.g.:
      \begin{itemize}
        \item \textit{what's the~weather like where you are?} (ask\_weather)
        \item \textit{what is rasa actually} (ask\_whatisrasa)
        \item \textit{yes please!} (affirm)
        \item \textit{i need help setting up} (install\_rasa)
        \item \textit{where is mexico?} (out\_of\_scope)
      \end{itemize}
      \textit{For a~list of all intents, explained and accompanied with real examples from the~dataset, see \autoref{tab:sara-intent-list} in \autoref{chap:A-datasets}.}

      In the~early days of the~chatbot, it supported fewer intents, and several artificial examples per intent were first hand-crafted by Rasa employees to train the~initial version of Sara's intent classifier. After Sara was deployed, more examples were collected and annotated from conversations with the~website's visitors\footnote{To get consent for such use of the~conversations, each visitor was shown the~following before starting a~conversation with Sara: ``Hi, I'm Sara! By chatting to me you agree to our privacy policy.'', with a~link to \rurl{rasa.com/privacy-policy/}}. Inspired by the~topics that people tended to ask about, new intent categories were added. Today, the~dataset still evolves and can be found -- together with the~implementation of Sara -- at \rurl{github.com/RasaHQ/rasa-demo}. It contains both the~original hand-crafted examples as well as the~(much more abundant) examples from real conversations.

      The~Sara dataset version I~use dates back to October 2019, when I~obtained it from Rasa and pseudonymised the~data\footnote{As a~former employee of Rasa, I~got access to the~data under the~NDA I~had signed with the~company. I~had permission from Rasa to use the~pseudonymised data for this project; the~use complied with the~ethical approval process of Rasa.}. In particular, I~removed any names of persons and e-mail addresses in any of the~examples, replacing them with the~special tokens \verb|__PERSON_NAME__| and \verb|__EMAIL_ADDRESS__|, respectively.      
      The~dataset comprises roughly 4,800 examples overall, and was originally split into 1,000 testing examples and 3,800 training examples. 
      I~further split the~training partition into training and evaluation, with roughly 2,800 and 1,000 examples, respectively. All three partitions have the~same class distribution.

      In line with how the~dataset is used for research at Rasa, I~use as the~main scoring metric the~\textit{multi-class micro-averaged F1 score} ($\text{F1}_{micro}$), even though other reasonable metrics exist. First of all, in the~binary classification case, the~F1 score balances two desirable properties of any classifier: precision $P$ and recall $R$: $\text{F1} = \frac{2 P R}{P+R}$.
      $P$ quantifies the~purity of reported positives: $P=TP/(TP+FP)$, $R$ quantifies the~reported portion of all positives: $R=TP/(TP+FN)$ (where $TP$ are true positives, $FP$ are false positives, and $FN$ are false negatives).
      In classification with more than 2 classes, one can still compute the~F1 score with respect to each individual class (treating the~multi-class classification as a~collection of binary classification decisions). Taking the~average of such class-specific F1 scores leads to the~\textit{macro-averaged} F1 metric:
      \begin{equation}
        \text{F1}_{macro} = \frac{1}{C}\sum_c \text{F1}_c = \frac{1}{C}\sum_c \frac{2 P_c R_c}{P_c+R_c}, \ \  C\ \text{being the~number of classes}
      \end{equation}
      While this metric quantifies the~F1 score on an~``average'' class, it does not account for different class sizes. In particular, if there are many small classes with little data to learn from and hence with low F1 scores, then the~average F1 will be pulled down -- even if the~classifier succeeds on most data, which belongs to several big classes.
      One way to deal with these undesirable effects of class imbalance is to use the~\textit{micro-averaged} F1 score. As its name suggests, it can be thought of as F1 averaged not on the~macro level (classes), but on the~micro level (individual examples), where the~F1 score for a~single example is 1 for a~correct prediction (this follows from the~standard formula $\text{F1} = \frac{2 P R}{P+R}$) and 0 for an~incorrect prediction (by definition):
      \begin{equation}
        \text{F1}_{micro} = \frac{1}{N}\sum_n \text{F1}_n = \frac{1}{N}\sum_n {\begin{cases}\text{if correct} & 1\\ \text{else} & 0\end{cases}}\ ,\ \ \  N\ \text{being the~number of examples}
      \end{equation}
      This score does take into account class imbalance because each example has ``one vote'' in the~averaging process. Therefore, it is well suited for situations where the~classifier should perform well on many examples, not necessarily on many classes (as there can be many classes that are insignificant).
      Additionally, the~$\text{F1}_{micro}$ score has the~same value as accuracy.
    }
  }

  \section{Data augmentation for larger transfer datasets}{
    \label{sec:augmentation}
    As discussed in \autoref{sec:kd-nlp}, knowledge distillation works best with large amounts of data used as the~transfer datasets. When the~transfer dataset is small, it does not provide enough opportunity for the~teacher to ``demonstrate its knowledge'' to the~student, and the~student learns little. Therefore, for each downstream task, I~create a~large transfer dataset by ``inflating'' the~small training portion of the~corresponding downstream dataset -- by augmenting it with additional sentences. I~then add teacher logits to such augmented dataset, and use it to train the~student models.

    \citet{Tang_2019b} demonstrated on several GLUE tasks that using an~augmented training portion for distillation leads to much better student performance than using just the~original small training portion.
    For CoLA in particular, using just the~small original training set led to very poor student performance (see Table~1 in \citeauthor{Tang_2019b}).
    
    I~take the~augmentation approach that \citeauthor{Tang_2019b} found to work the~best: Generating additional sentences using a~GPT-2 model \citep{Radford_2019} fine-tuned on the~training set\footnote{I used the~code for \citet{Tang_2019b} which is available at \rurl{github.com/castorini/d-bert}.}. The~steps for creating the~transfer dataset from the~training portion are:
    \begin{enumerate}
      \item Fine-tune the~pre-trained GPT-2 model (the~345-million-parameter version) on the~training portion for 1 epoch (where an~epoch is one complete pass through all training examples) with the~language-modelling objective (i.e. predicting the~next subword token given the~sequence of tokens so far).
      \item Sample from the~model a~large number of tokens to be used as the~beginnings (\textit{prefixes}) of the~augmentation sentences. This sampling can be done as one-step next-token prediction given the~special \verb|SOS| (start-of-sentence) token.
      \item Starting from each sampled prefix, generate an~entire sentence token by token by repeatedly predicting the~next token using the~GPT-2 model. The~generation of a~sentence stops when the~special \verb|EOS| (end-of-sentence) token is generated or when the~desired maximum sequence length is reached -- in this case 128 tokens.
      \item Add the~generated augmentation sentences to the~original training data, and generate the~teacher logits for each sentence.
    \end{enumerate}

    For consistency with \citet{Tang_2019b}, I~added 800,000 augmentation sentences to the~training data of each of the~three downstream tasks, resulting in the~transfer datasets comprising roughly 808,500, 867,000, and 802,800 sentences for CoLA, SST-2, and Sara, respectively.
  }

  \section{Probing tasks}{
    \label{sec:probing-tasks}
    The~probing tasks (discussed in \autoref{sec:understanding-models}) I~use after knowledge distillation to analyse the~linguistic capabilities of the~students and the~teacher. In particular, I~use the~probing suite curated by \citet{Conneau_2018}, consisting of 10 tasks\footnote{The~data, along with code for probing neural models, are publicly available as part of the~SentEval toolkit for evaluating sentence representations \citep{SentEval-paper} at \rurl{github.com/facebookresearch/SentEval}.}.

    \begin{figure}[h!t]
      \centering
      \includegraphics[width=11cm]{graphics/probing-scheme}
      \caption{A high-level diagram of the~probing process.}
      \label{fig:probing-scheme}
    \end{figure}

    Each probing task is a~collection of 120,000 labelled sentences, split into training (100,000), evaluation (10,000) and test (10,000) set. The~label refers to a~property of the~sentence, such as the~sentence's length. The~aim is to recover the~property from an~encoding of the~sentence, produced by the~model being probed. \autoref{fig:probing-scheme} shows the~basic workflow. First, the~model is used to produce an~encoding of each sentence. Then, a~light-weight classifier is trained, taking the~training sentences' encodings as inputs and learning to produce the~labels. The~evaluation sentence encodings are used to optimise the~hyperparameters of the~classifier. Finally, a~probing score (accuracy) is produced on the~test encodings. The~score quantifies how well the~sentence property in question is recoverable (and thus present) in the~encodings. This serves as a~proxy measure of the~linguistic knowledge tied to the~property. If, for instance, the~property to be recovered is the~depth of a~sentence's syntactic parse tree, the~score hints at the~model's (un)capability to understand (and parse) the~syntax of input sentences.
    By extracting probing encodings from different parts of a~model (e.g. from different layers), the~probing scores can additionally serve as cues for localising the~linguistic knowledge in question -- one can observe how the~amount of this knowledge varies across different model parts and where it is most concentrated.

    Regarding the~linguistic capabilities explored by the~probing suite, each task falls into one of three broad categories -- surface properties, syntax, and semantics:
    \begin{enumerate}
      \item {Surface information:
        \begin{itemize}
          \item \textbf{Length} is about recovering the~length of the~sentence. The~labels are somewhat simplified: The~actual sentence lengths grouped into 6 equal-width bins -- making this task a~6-way classification.
          \item \textbf{WordContent} is about identifying which words are present in the~sentence. A~collection of 1000 mid-frequency words was curated, and sentences were chosen such that each contains exactly one of these words. The~task is to identify which one (1000-way classification).
        \end{itemize}
      }
      \item{Syntactic information:
        \begin{itemize}
          \item \textbf{Depth} is about classifying sentences by their syntactic parse tree depth, with depths ranging from 5 to 12 (hence 8-way classification).
          \item \textbf{BigramShift} is about sensitivity to (un)natural word order -- identifying sentences in which the~order of two randomly chosen adjacent words has been swapped (binary classification). While syntactic cues may be sufficient to identify an~unnatural word order, intuitively, broken semantics can be another useful signal -- thus making this task both syntactic and semantic.
          \item \textbf{TopConstituents} is about recognising the~top syntactic constituents -- the~nodes found in the~syntactic parse tree just below the~S (sentence) node. This is framed as 20-way classification, choosing from 19 most common top-constituent groups + the~option of ``other''.
        \end{itemize}
      }
      \item{Semantic information:
        \begin{itemize}
          \item \textbf{Tense} is a binary classification task, identifying the~tense (present or past) of the~sentence's main verb (the~verb in the~main clause). At the~first sight, this is mainly a~morphological task (in English, most verbs have the~past tense marked by the~``-d/ed'' suffix). However, the~model first has to identify the~main verb within a~sentence, which makes this task also syntactic and semantic.
          \item \textbf{SubjNumber} is about determining the~number (singular or plural) of the~sentence's subject (binary classification). Similar to the~previous task, this one (and the~next one too) is arguably about both morphology and syntax/semantics.
          \item \textbf{ObjNumber} is the~same as SubjNumber, applied to the~direct object of a~sentence.
          \item \textbf{OddManOut} is binary classification, identifying sentences in which a~randomly chosen verb or noun has been replaced with a~different random verb or noun. Presumably, the~random replacement in most cases makes the~sentence semantically unusual or invalid (e.g. in ``He reached inside his persona and pulled out a~slim, rectangular black case.'' the~word ``persona'' is clearly odd). To make this task more difficult, the~replacement word is chosen such that the~frequency of the~bigrams in the~sentence stays roughly the~same. (Otherwise, in many cases, the~random replacement would create easy hints for the~probing classifier, in the~form of bigrams that are very unusual.)
          \item \textbf{CoordinationInversion} works with sentences that contain two coordinate clauses (typically joined by a~conjunction), e.g. ``\underline{I ran to my dad}, but \underline{he was gone}.'' In half of the~sentences, the~order of the~two clauses was swapped, producing sentences like: ``\underline{He was gone}, but \underline{I ran to my dad}.'' The~task is to identify the~changed sentences (which are often semantically broken).
        \end{itemize}
      }
    \end{enumerate}

    When choosing from the~existing probing suites, I~considered that of \citet{Tenney_2019b} as well. As the~authors showed, their tasks and methods can effectively localise different types of linguistic knowledge in a~Transformer model like BERT.
    However, the~task data are not freely available, the~tasks have a~relatively narrow coverage with a heavy focus on the~most complex NLP tasks like entity recognition and natural language inference, and the~probing is done on single-token representations.
    The~suite of \citeauthor{Conneau_2018}, on the~other hand, is publicly available, better covers the~easier tasks (surface and syntactic information), and examines whole-sentence representations.
    One interesting direction for future work is to use both of these probing suites, compare the~results they lead to (in particular their agreement), and explore the~extent to which the~different probing approaches complement each other.
  }

  \section{Summary}{
    I~have introduced the~three different types of data used in this work. These types also define the~skeleton of my experiments and analyses:
    \begin{enumerate}
      \item First, I~train one teacher model for each of the~three downstream datasets.
      \item Then, each teacher teaches two students, using the~transfer dataset as the~``carrier'' of the~teacher knowledge.
      \item Finally, the~linguistic skills of the~students as well as the~teachers are measured and analysed using the~probing tasks.
      \item Additionally, the~downstream task sentences are used for analysing the~prediction characteristics of each model.
    \end{enumerate}
    While using the~GLUE benchmark tasks is the~usual way of comparing and analysing sentence encoder models, none of the~tasks focuses on the~conversational domain. I~use an~additional downstream task -- Sara -- to make this work more relevant for the~area of conversational AI.
    My prior familiarity with the~Sara dataset can be an~advantage when later analysing the~individual predictions of the~teacher and student models on the~downstream datasets.
  }
}

\chapter{Methods and Implementation}{
  \label{chap:methods-implementation}

  This chapter elaborates on the~main objectives of this work, the~knowledge distillation and model analysis approaches I~took, and goes into detail in describing the~design and implementation work underlying my experiments.

  \section{Methods and objectives}{
    \label{sec:methods}
    % main aim: explore KD by exploring models (teacher, students). in the~context of multiple tasks (for coverage)
    The~main aim is to explore the~use of knowledge distillation. In particular, it is used on three different NLP tasks (CoLA, SST-2, Sara) and with two different student architectures: a~bidirectional LSTM student and a~BERT student. An~analysis stage follows, where I~look at and compare the~teacher and students on each task. 
    Note that the~focus is not on improving scores reported by previous works, or on finding the~best hyperparameter configurations; I~aim to learn more about knowledge distillation.

    Being inspired by my internship at Rasa on compressing BERT\footnote{See \rurl{blog.rasa.com/compressing-bert-for-faster-prediction-2/} and \rurl{blog.rasa.com/pruning-bert-to-accelerate-inference/}.}, this work aims to produce student models as small as possible.
    Therefore, I~take the~approach of first fine-tuning a~teacher model and then distilling the~fine-tuned knowledge into small students (for the~other option, refer back to the~discussion in \autoref{sec:kd-nlp}).

    % naturally involves KD first and I~did basic optimisation to arrive at students small but good
    Creating small yet well-performing students requires not just setting up an~implementation of knowledge distillation, but also optimising the~student models' hyperparameters.
    Even if extensive optimisation is not the~main goal, the~models used for further analysis should reach reasonable performance levels in order for the~analysis to be of real value.
    % However, the~hyperparameter exploration is kept to a~minimum where possible.
    % initial optimisation done on CoLA, only minimal optimisation on SST-2 and Sara (to not end up in a~rabbit hole)
    However, in order to constrain the~amount of optimisation, I~carry out a~relatively thorough hyperparameter exploration only on the~CoLA task. 
    Subsequently, the~best parameters are applied on the~other two tasks, with only the~most essential decisions -- like the~student model size -- made on each task separately.

    % real work begins once students are trained: analysing what they learned well and what not, how they differ from teacher and from each other. trying to generalise across the~tasks, but each task is different and can provide different insights.
    The~analysis stage of this work inspects what the~two students learnt well and what they did not, how they differ from their teacher and from each other. 
    Where possible, I~try to produce conclusions that generalise across the~three downstream datasets.

    % first probing (easier to carry out, quantitative)
    As the~first analysis approach, all models are probed for various types of linguistic knowledge. This produces simple, quantitative results, which, however, are not necessarily easy to interpret.

    % then prediction analysis (mostly qualitative), looking at both correctness and confidence. done on eval set because that one is labelled for GLUE
    As the~second approach, I~carry out a~-- mostly qualitative -- analysis of the~models' predictions on concrete sentences. 
    This approach is not widely reported, despite being simple in nature -- manually inspecting a~model's predictions on a~case-by-case basis follows from the~natural curiosity of an~empirical scientist.
    While it involves a~lot of human labour and does not guarantee easy-to-interpret, quantitative results, I~still make use of this approach and try to gain qualitative insights.
    In particular, the~predictions are inspected both in terms of correctness -- e.g. manually analysing sentences which were classified correctly by one model but not by another -- and through confidence -- which models are more confident, on what sentences are they (un)confident, and how this relates to their (in)correctness.

    Finally, the~results of probing and prediction analysis are juxtaposed. I~ask whether the~two approaches agree or disagree, and whether they shed light on the~same or different aspects of the~models and of knowledge distillation.

    \textit{Because of the~unavailability of test-set labels in CoLA and SST-2, the~prediction analysis is carried out on the~evaluation set for each downstream task. This can be understood as inspecting the~model qualities being optimised when one tunes a~model's hyperparameters on the~evaluation data. Another option would be to carry out the~analysis on a~held-out set not used in training.}
  }

  \section{System overview and adapted implementations}{
    Because a~lot of research around Transformers is open-sourced, my work makes use of multiple existing codebases. \autoref{fig:pipeline} shows the~high-level pipeline of this project. It is inspired by the~best pipeline of \citet{Tang_2019b}, although they only used the~BiLSTM student and did not carry out probing or prediction analysis.
  
    \begin{figure}[h!t]
      \makebox[\textwidth][c]{\includegraphics[width=17.5cm]{graphics/pipeline}}
      \cprotect\caption{The~main pipeline of this work: \circled{1} teacher fine-tuning, \circled{2} GPT-2 fine-tuning, \circled{3} generating augmentation sentences, \circled{4} adding teacher logits to the~augmented training dataset, \circled{5} knowledge distillation into students, \circled{6} producing probing sentence encodings, \circled{7} training the~probing classifier and producing probing scores, \circled{8} producing predictions on evaluation sentences.}
      \label{fig:pipeline}
    \end{figure}

    For most of the~implementation, the~\verb|transformers| open-source PyTorch library \citep{Wolf_2019}\footnote{\rurl{github.com/huggingface/transformers}}, is used, which provides tools for working with pre-trained Transformers like BERT. For knowledge distillation, I~adapt the~code of \citet{Sanh_2019}, which is today also part of \verb|transformers|\footnote{\rurl{github.com/huggingface/transformers/tree/master/examples/distillation}}. (Note that the~authors apply knowledge distillation \textit{before} downstream fine-tuning.) For augmenting the~training data using GPT-2 and for knowledge distillation with the~BiLSTM student, I~adapt the~code of \citeauthor{Tang_2019b}\footnote{\rurl{github.com/castorini/d-bert}}, which uses an~early version of \verb|transformers|. For probing the~two students, the~SentEval framework \citep{SentEval-paper}\footnote{\rurl{github.com/facebookresearch/SentEval}} is used.

    My own contributions to the~implementation lie primarily in adapting and integrating the~different codebases into one, and in adding the~possibility for optimising a~range of student hyperparameters. I~also make the~code more flexible, relative to the~original codebases which encode numerous fixed design decisions made by \citeauthor{Sanh_2019} and \citeauthor{Tang_2019b}. The~core of my implementation is open-sourced as a~fork of the~\verb|transformers| library at \rurl{github.com/samsucik/pytorch-transformers/}, while the~implementation needed for individual experiments, analyses, and reporting, resides at \rurl{github.com/samsucik/knowledge-distil-bert}.
  }

  \section{Implementation details}{
    \label{sec:implementation-details}
    % teacher fine-tuning
    \subsection{Teacher fine-tuning}{
      Following \citet{Tang_2019b}, the~case-insensitive pre-trained BERT\textsubscript{Large} is used as the~teacher model (from now, referred to as \BERTT).
      With $L=24$ encoder layers, $A=16$ self-attention heads, the~hidden dimension $d_h=1024$ and the~intermediate dimension $d_I=4096$, the~model has 340 million trainable parameters (as discussed in more detail previously in \autoref{sec:BERT}).
      The~large BERT variant generally performs better than the~110-million-parameter BERT\textsubscript{Base} variant (both variants published by \citet{Devlin_2018}) and is therefore more attractive, but also slower, with a~greater incentive for compression.

      \begin{table}[h!t]
      \centering
      \footnotesize
      \begin{tabular}{m{0.22\textwidth}m{0.4\textwidth}}
      \toprule
      loss function & cross-entropy \\
      \hline
      learning algorithm & Adam ($\eta=5\times10^{-5}$, $\beta_1=0.9$, $\beta_2=0.999$) \\
      \hline
      training budget & 3 epochs \\
      \hline
      $\eta$ scheduling & linear warm-up (first 10\% of training), then linear decay (see \autoref{fig:teacher-fine-tuning}) \\
      \hline
      batch size & 36 \\
      \hline
      dropout rate & 0.1 \\
      \bottomrule
      \end{tabular}
      \caption{The~fine-tuning configuration of the~teacher BERT model.}
      \label{tab:initial-config-teacher}
      \end{table}

      For teacher fine-tuning on each downstream task, the~procedure of \citeauthor{Tang_2019b} is used, summarised in \autoref{tab:initial-config-teacher}.
      While the~performance of \BERTT~converges (flattens) within the~3-epoch training budget on CoLA and SST-2, the~convergence is much slower for Sara. Hence, I~empirically found a~more suitable number of epochs within which the~teacher converges on Sara: 10. See \autoref{fig:teacher-fine-tuning} for the~evaluation-set performance of the~teacher models and how they converge during fine-tuning.
      \begin{figure}[h!t]
        \centering
        \makebox[\textwidth][c]{\includegraphics[width=17cm]{../experiments/analysis/img/teacher-fine-tuning}}
        \caption{The~evaluation-set performance of teacher models across fine-tuning, together with an~illustration of the~learning rate scheduling for \BERTT~on Sara. Unintentionally, I~used different logging frequencies in fine-tuning the~teachers, hence the~SST-2 plot is dense (and appears more noisy) while the~CoLA plot is sparse.}
        \label{fig:teacher-fine-tuning}
      \end{figure}
    }

    % GPT-2 augmentation
    \subsection{Augmentation with GPT-2}{
      In fine-tuning the~GPT-2 model, again the~procedure of \citeauthor{Tang_2019b} is used (summarised in \autoref{tab:config-gpt-2}). This is very similar to the~fine-tuning configuration used for \BERTT, with small differences. The~AdamW learning algorithm is used \citep{Loshchilov_2019}, which is a~variant of Adam with weight decay imposed on all learnable parameters, making the~values slowly decay towards 0 in the~absence of learning. The~decay rate $\lambda$ determines the~fraction by which each weight decays at each training step.
      The~only parameter I~choose differently from \citeauthor{Tang_2019b} is the~batch size $B$: While they use batches of 48 examples, I~only process examples in batches of 16, in order to make the~fine-tuning possible with the~limited memory resources.

      \begin{table}[h!t]
      \centering
      \footnotesize
      \begin{tabular}{m{0.22\textwidth}m{0.33\textwidth}}
      \toprule
      loss function & cross-entropy \\
      \hline
      learning algorithm & AdamW ($\eta=5\times10^{-5}$, $\beta_1=0.9$, $\beta_2=0.999$, $\lambda=1\times10^{-3}$) \\
      \hline
      training budget & 1 epoch \\
      \hline
      $\eta$ scheduling & linear warm-up (first 10\% of training), then linear decay \\
      \hline
      batch size & 16 \\
      \hline
      dropout rate & 0.1 \\
      \bottomrule
      \end{tabular}
      \caption{The~fine-tuning configuration of the~GPT-2 model.}
      \label{tab:config-gpt-2}
      \end{table}
    }

    % student architectures + default params
    \subsection{BiLSTM student model}{
      \label{sec:student-bilstm}
      As the~first student, I~use the~bidirectional LSTM (BiLSTM) from \citeauthor{Tang_2019b} (see \autoref{fig:bilstm}). The~model comprises in particular one hidden BiLSTM layer with 300 units, which is composed of two LSTM layers processing the~inputs in opposite directions. The~last hidden states for either of the~two processing directions are concatenated and passed to a~fully connected layer with 400 output units\footnote{Even though \citeauthor{Tang_2019b} tried also other, slightly different layer dimensions, these are the~ones that worked the~best on CoLA.}, which uses the~rectified linear unit (ReLU) activation function \citep{Nair_2010}, and dropout. A~final (linear) layer follows, projecting to the~number of target classes, i.e. producing the~logits. The~model is topped with a~softmax classifier for normalising the~logits and producing class probabilities.
      \begin{figure}[h!t]
        \centering
        \includegraphics[width=7cm]{graphics/bilstm}
        \caption{The~bidirectional LSTM student. Diagram adapted from Figure 1 of \citet{Tang_2019a}.}
        \label{fig:bilstm}
      \end{figure}

      The~original model was built to process sentences word by word, encoding each word using the~pre-trained word2vec embeddings\footnote{The~300-dimensional version trained on Google News, see \rurl{code.google.com/archive/p/word2vec/}.} before passing it to the~LSTM layer. Words for which there is no embedding (\textit{out-of-vocabulary} words, or just OOV) are embedded using a~vector initialised with random numbers drawn uniformly from [-0.25, 0.25]. The~embedding layer supports three \textit{embedding modes}, based on \citet{Kim_2014}:
      \begin{enumerate}
        \item \textbf{Static}: the~embedding parameters are frozen and do not change during training.
        \item \textbf{Non-static}: the~embedding parameters are allowed to change (fine-tune) during training.
        \item \textbf{Multichannel}: two embedding instances are used in parallel, one is frozen, the~other one is allowed to change. For each input word, the~two embeddings produced are concatenated together for further processing. The~multichannel mode is the~one used by \citeauthor{Tang_2019b}.
      \end{enumerate}

      One significant change I~made to this model is enabling the~use of wordpiece embeddings instead of word-level ones. This way, the~fine-tuned embedding parameters from \BERTT~can be used to initialise the~student's embedding layer, providing some of the~teacher's ``knowledge'' even before the~student training (knowledge distillation) begins.

      When the~word2vec embeddings are used, the~embedding matrix of the~LSTM is constructed in the~following way:
      \begin{enumerate}
        \item A~vocabulary of all distinct words present in the~transfer dataset is made.
        \item Only the~word2vec vectors corresponding to these words are taken and put together to create the~embedding matrix.
      \end{enumerate}
      This way, even though the~full word2vec collection covers 3,000,000 words, the~word-level embedding matrix (whether used by the~LSTM student or the~BERT student) has fewer entries. For the~particular transfer datasets I~use, the~vocabulary has 243,120 words for CoLA, 284,273 words for SST-2, and 172,183 words for Sara.

      In total, this model -- from now referred to as \LSTMS~-- has 2.41 million trainable parameters (excluding the~embedding layer -- this is how model sizes are reported everywhere further in this work\footnote{In line with how the~number of model parameters is reported by others, for instance in \citet{Tang_2019b,Tang_2019a}, but also in the~GLUE benchmark leaderbord. One reason for this is that the~number of embedding parameters is not directly related to the~model size and mostly depends on the~type of embeddings used -- the~embedding vocabulary size and the~dimensionality of each embedding.}), making it 140x smaller than \BERTT.
    }

    \subsection{BERT student model}{
      \label{sec:student-bert}
      For the~second student, a~down-scaled version of BERT\textsubscript{Large} is used, matched for size with \LSTMS. In particular, I~scale all the~dimensions of BERT\textsubscript{Large} down by a~factor of \mytilde5, leading to a~smaller BERT with $L=5$ encoder layers, the~hidden dimension $d_h=204$, the~intermediate dimension $d_I=750$, and $A=3$ self-attentional heads -- amounting to 2.42 million trainable parameters (embedding parameters excluded). This model is from now referred to as \BERTS.
    }

    % knowledge distillation + default params 
    % mention that KD is like normal training given the~transfer set
    \subsection{Knowledge distillation}{
      While \autoref{tab:initial-configs} summarises the~initial configuration of both student models, I~elaborate more on these parameters in the~rest of this section.

      During knowledge distillation, \BERTS~is trained using the~cross-entropy loss. The~softmax temperature is fixed at $T=3$.\footnote{Usual values are from 1 (no effect) to 3. For instance, \citet{Sanh_2019} use $T=2$. In a~work that is much closer to my situation, \citet{Tsai_2019} apply knowledge distillation from BERT\textsubscript{Base} into a~18-million-parameter smaller BERT, observing that from $T=\{1, 2, 3\}$ the~best one was $T=3$.}
      
      Originally, both students were implemented to use random initialisation from scratch before training, with the~exception of the~embedding layer of \LSTMS, which was initalised from word2vec. Later, I~explore different ways of initialising the~embedding layers.
      
      \LSTMS~uses the~mean squared error (MSE) loss, following \citeauthor{Tang_2019b} who report that MSE led to slightly better performance (compared to cross-entropy loss with $T=3$). Following preliminary experiments on CoLA, I~set the~training budget to 30 epochs for \LSTMS~(same as \citeauthor{Tang_2019b}). \BERTS~converges slower and therefore uses a~60-epoch training budget in all following experiments. In student training, the~evaluation-set performance reported is always for the~best model checkpoint as observed during training; in particular, it may not be the~final model version.
      Using this approach, even if a~student's performance eventually starts to decrease during training, the~best-performing version is retained for further analysis and comparison.

      % The~learning algorithm is Adam for \BERTS, with $\beta_1=0.9$ and $\beta_2=0.98$ (the~defaults in \verb|transformers|). The~learning rate was initially set to $5\times10^{-4}$ and later optimise.
      Following \citeauthor{Tang_2019b}, the~Adadelta learning algorithm \citep{Zeiler_2012} with $\eta=1.0$ and $\rho=0.95$ is used for training \LSTMS, while Adam is used for \BERTS.
      Note that Adam is an~improved successor of Adadelta\footnote{In particular, the~adaptive mechanism of Adadelta considers only the~recent squared gradient magnitudes, whereas Adam also considers the~simple (not squared) gradients.} and is much more widely used; later in this work, I~explore the~use of Adam for \LSTMS. No $\eta$ scheduling is used with \LSTMS, while \BERTS~uses scheduling similar to \BERTT.
      % For \BERTS, I~originally anneal $\eta$ linearly from 0 over the~first 10 epochs, and let it linearly decay to 0 during the~remaining 50 epochs. Later, I~explore other annealing schedules.
      To prevent ``gradient explosion'' in \LSTMS, the~total magnitude of all gradients is clipped to 30.0 before every parameter update (as used by \citeauthor{Tang_2019b}); however, throughout all experiments, I~never observe the~gradient norm to reach this limit.
      For more robust training, the~standard dropout rate of 0.1 is used during training of both students, following \citet{Devlin_2018} and \citeauthor{Tang_2019b}.
      \citeauthor{Tang_2019b} report the~small batch size $B=50$ to work well with the~BiLSTM student. For \BERTS, I~initially use a~larger batch size $B=256$.

      While LSTMs can process sequences of any lengths, Transformer models like BERT impose a~maximum sequence length for practical reasons, with all sequences within a~batch padded to the~maximum length. Although \BERTT~allows sequences of up to 512 wordpieces in length, extremely few sentences reach this length -- especially in this work, where all inputs are single sentences, not sentence pairs. Therefore, to accelerate training, I~use the~maximum sentence length of 128 tokens for \BERTS.

      \begin{table}[h!tb]
      \centering
      \footnotesize
      \begin{tabular}{P{0.18\textwidth} m{0.3\textwidth} m{0.3\textwidth} }
      \toprule
      & \LSTMS & \BERTS \\
      \toprule
      loss function & mean square error & cross-entropy, $T=3$ \\
      \hline
      learning algorithm & Adadelta ($\eta=1.0$, $\rho=0.95$) &  Adam ($\eta=5\times10^{-4}$, $\beta_1=0.9$, $\beta_2=0.98$) \\
      \hline
      training budget & 30 epochs & 60 epochs \\
      \hline
      $\eta$ scheduling & none & linear warm-up (10 epochs), then linear decay \\
      \hline
      batch size & 50 & 256 \\
      \hline
      embedding layer initialisation & word2vec & wordpiece, from \BERTT \\
      \bottomrule
      \end{tabular}
      \caption{The~initial parameters of both student models.}
      \label{tab:initial-configs}
      \end{table}
    }

    % probing
    \subsection{Probing}{
      \label{sec:implementation-details-probing}
      For the~light-weight probing classifier, \citet{Conneau_2018} use a~small neural network comprising one hidden layer with the~sigmoid activation function and dropout, followed by a~linear layer projecting to the~desired number of classes. 
      In training the~classifier, early stopping is used, i.e. training stops when the~evaluation-set accuracy does not improve over 5 consecutive iterations.
      For consistency with the~exact method of \citeauthor{Conneau_2018}, I~tune the~dropout rate (choosing from [0.0, 0.1, 0.2]) and the~hidden layer width (choosing from [50, 100, 200]) using the~evaluation set. Each probing score is reported as the~one for the~best dropout and layer width values.
      \autoref{tab:config-probing-classifier} summarises all important training parameters.

       % The~cross-entropy loss is used, with the~Adam learning algorithm with $\eta=0.001$, $\beta_1=0.9$ and $\beta_2=0.999$. 
      \begin{table}[h!t]
      \centering
      \footnotesize
      \begin{tabular}{m{0.19\textwidth} m{0.4\textwidth}}
      \toprule
      loss function & cross-entropy \\
      \hline
      learning algorithm & Adam ($\eta=1\times10^{-3}$, $\beta_1=0.9$, $\beta_2=0.999$) \\
      \hline
      training budget & 4 epochs (early stopping) \\
      \hline
      batch size & 64 \\
      \hline
      dropout rate &  [0.0, 0.1, 0.2]\\
      \bottomrule
      \end{tabular}
      \caption{The~training configuration of the~probing classifier.}
      \label{tab:config-probing-classifier}
      \end{table}

      When probing a~model, an~important design decision is how to extract sentence representations from the~model's layers.
      The~BiLSTM layer of \LSTMS~can produce at each timestep two hidden states (one for each processing direction). \citeauthor{Conneau_2018} experiment with:
      \begin{enumerate}
        \item Creating a~\textit{BiLSTM-max} encoding such that each of its elements is the~maximum over the~values for each timestep. (The~encoding has the~same dimensionality as the~BiLSTM layer output.)
        \item Creating a~\textit{BiLSTM-last} encoding by simply taking the~last hidden state in each direction -- the~encoding is the~same as the~BiLSTM layer output.
      \end{enumerate}
      \citeauthor{Conneau_2018} report mixed results, with BiLSTM-max encodings leading to better probing scores on some of the~probing tasks. I~am constrained to using BiLSTM-last since the~PyTorch implementation of LSTMs does not give access to intermediate hidden states, only to the~last one in each direction.

      In BERT, all hidden representations produced by each encoder layer can be accessed. I~try three different ways of combining a~sequence of hidden representations from a~particular layer into a~single encoding:
      \begin{enumerate}
        \item Maximum pooling, equivalent to BiLSTM-max: Taking the~maximum value for each element over all hidden representations.
        \item Single-position encoding (the~equivalent of BiLSTM-last): Taking the~hidden representation that is used for the~final classification. While in \LSTMS, this would mean taking the~last hidden state in each direction, in BERT, it is the~hidden representation of the~first token (the~special \verb|[CLS]| token).
        \item Average pooling (not explored by \citeauthor{Conneau_2018}): Similarly to maximum pooling, this uses the~average of each element across all representations.
      \end{enumerate}
      After conducting simple preliminary probing experiments with \BERTT~on each downstream task, I~observed that the~differences between the~three approaches are mostly inconsistent and small. However, in many cases, maximum pooling produced worse probing scores than the~other two techniques, and average pooling slightly outperformed single-position representations. In all further probing experiments with \BERTT~and \BERTS, the~average pooling approach is used.

      Inspired by the~localisation experiments of \citet{Tenney_2019b}, I~probe various encoder layers across the~BERT models in order to also localise each type of language knowledge within the~model's architecture.
    }
  }

  \section{Computing environment and runtimes}{
    All major parts of my experiments -- teacher and GPT-2 fine-tuning, augmentation data sampling, teacher logits generation, knowledge distillation and probing -- are run in the~environment of the~University's Teaching cluster\footnote{\rurl{computing.help.inf.ed.ac.uk/teaching-cluster}.}.

    Each job uses its own one Nvidia GPU -- either GeForce GTX TITAN X or GeForce RTX 2080 Ti -- with 12-13GB of memory, and additional 30GB of RAM for use with CPU processes.

    \begin{table}[h!t]
    \centering
    \footnotesize
    \begin{tabular}{m{0.27\textwidth}ccc}
    \toprule
    & CoLA & SST-2 & Sara \\
    \toprule
    teacher fine-tuning & $\sim$30min & $\sim$4h & $\sim$55min \\
    \hline
    GPT-2 fine-tuning & 3min & 31min & 1min \\
    \hline
    augmentation data sampling & 17h & 15h & 4h \\
    \hline
    teacher logits generation & $\sim$1h & $\sim$8h & $\sim$2h \\
    \hline
    \LSTMS~training & $\sim$5h & $\sim$8h & $\sim$6h \\
    \hline
    \BERTS~training & $\sim$15h & $\sim$26h & $\sim$22h \\
    \bottomrule
    \end{tabular}
    \caption{The~runtimes for all steps of knowledge distillation with augmented transfer datasets.}
    \label{tab:runtimes}
    \end{table}

    All important runtimes are reported in \autoref{tab:runtimes}\footnote{Note that the~only processes that are parallelised are the~augmentation data sampling and the~teacher logits generation -- both use 4 parallel threads, each with its own GPU with 6GB of memory. In logits generation, examples are processed in batches of 2048 in each thread.}.
    The~reason why most steps take the~longest on SST-2 is 1) the~amount of training data (almost 10x more than for CoLA), and 2) the~fact that sentences in SST-2 are longer than those in CoLA and Sara.
    Interestingly, even though \LSTMS~and \BERTS~are of similar size, the~BERT model takes much longer to train -- likely because it is much deeper.

    Because of the~role restrictions in the~cluster, I~cannot run more than 20 jobs at the~same time. This has a~significant impact especially on the~time it takes to run the~hyperparameter exploration experiments (see the~next chapter). 
    It is also the~main reason why I~do not -- with a~few exceptions -- repeat experiments with varying random seeds for more robust results. 

    % CoLA: end of epoch 1 | loss 0.86 | ppl  89.79 | bpc 1.237 | 02:56
    % SST-2:end of epoch 1 | loss 0.90 | ppl  75.52 | bpc 1.296 | 31:02
    % Sara: end of epoch 1 | loss 0.94 | ppl 111.48 | bpc 1.363 | 00:55
  }

  \section{Summary}{
    % aims: insights, nt numbers
    In this chapter, I~presented the~high-level set up as well as the~implementation details of all experiments.
    % implementation: work of others
    Importantly, the~main outcomes of this exploratory work are intended to be insights, not improved performance scores.
    % my own work: combining; introducing small bert; and analysis (probing across layers; new probing encoding extraction technique!)
    With most of the~programming efforts going into adapting and integrating existing codebases, my original contributions are mostly intellectual: 
    Using two architecturally different students side by side; 
    using the~probing suite of \citet{Conneau_2018} for localisation of linguistic knowledge;
    using a~new technique for extracting probing encodings; 
    and later manually analysing the~predictions made by the~teacher and student models.
    % KD very slow, need to think twice before experiments
  }
}

\chapter{Training student models}{
  In this chapter, knowledge distillation is used to teach (train) student models \BERTS~and \LSTMS~from the~fine-tuned teacher \BERTT. This is done separately on each of the~three downstream tasks -- CoLA, SST-2, and Sara.
  The~objective is to obtain students which are small but perform well -- relative to the~teacher. Where possible, the~student size is not increased above the~initial dimensions outlined in \autoref{sec:student-bilstm} (\LSTMS) and \autoref{sec:student-bert} (\BERTS), which corresponds to keeping the~number of trainable parameters at \mytilde2.4 million.

  As discussed in \autoref{sec:methods}, my aim is not to find all the~best possible student hyperparameters, but I~still briefly explore some of them to gain an~intuition for the~reasonable ranges of values and for their behaviour in knowledge distillation.
  In particular, I~find a~well-performing training configuration of each student on CoLA, choosing based on the~model's evaluation-set score. Then, on the~remaining tasks, I~use the~same configuration, only tailoring a~small number of parameters to the~need of the~concrete dataset at hand. (Most importantly, the~student size is adjusted separately for each task because more difficult tasks may require larger (more complex) models for decent accuracy levels.)

  After obtaining well-performing students for each task, these are briefly compared with the~respective teacher in terms of model size, inference speed, as well as evaluation- and test-set scores.

  \section{Hyperparameter exploration}{
    \label{sec:hparam-general}

    The~initial exploration conducted on CoLA is restricted to these following essential hyperparameters in both students:
    \begin{enumerate}
      \item $\eta$ -- the~learning rate. For \LSTMS, also the~choice of a~learning algorithm (the~originally used Adadelta vs the~more general Adam).
      \item Learning rate scheduling: the~warmup duration (in epochs) $E_{w}$ of gradual warmup of $\eta$ \citep{Goyal_2017}, and the~optional use of linear decay of $\eta$ following after the~warmup period (for an example, refer back to \autoref{fig:teacher-fine-tuning}).
      \item $B$ -- the~minibatch size.
      \item Embedding type and mode -- word-level (initialised from word2vec) vs wordpiece (initialised from the~respective \BERTT\footnote{Initially, I~experimented with \LSTMS~using word2vec embeddings (as in \citet{Tang_2019b,Tang_2019a}) while the~embedding layer of \BERTS~was randomly initialised.
      However, this poses a~disadvantage for \BERTS~-- it starts with no initial knowledge, unlike the~BiLSTM student.
      To eliminate this disparity, \BERTS's wordpiece embeddings were initialised with the~teacher's wordpiece embedding parameters, and a~trainable linear layer was added in the~student to project these (high-dimensional) teacher embeddings to the~smaller hidden dimensionality of the~student $d_h=204$.
      (Note that the~token type embeddings and positional embeddings are not initialised from the~teacher and hence do not require the~linear transform for dimensionality reduction.
      Instead, these embeddings are added to the~wordpiece embeddings inside \BERTS~after the~wordpiece embeddings are dimensionality-reduced.)
      Even though the~idea of initialising one model with another one's parameters is not new, to the~best of my knowledge, I~am the~first one to initialise a~BERT student in knowledge distillation in this way.}), non-static vs multichannel\footnote{The~static mode (frozen embeddings, not allowed to be trained) was not tried, following preliminary experiments where freezing the~parameters led to very poor performance.}.
    \end{enumerate}

    The~parameters are optimised one at a~time, in the~order they are enumerated above. At each step, the~best value of one parameter is found and kept in all further steps. The~explored values as well as the~initial values (see \autoref{sec:implementation-details}) and the~discovered best values are shown in \autoref{tab:hparam-exploration}. Note that the~embedding type and mode is explored also later, separately for each task.
    For more details on how the~individual parameters were chosen, see \autoref{sec:A-hparam-exploration-cola} in \autoref{chap:A-student-training}.

    Most importantly, the~LSTM student is found to outperform \BERTS~and converge much faster. Additionally, the~LSTM prefers small batches (in line with the~findings of \citet{Tang_2019a}) and does not benefit from learning rate warmup (unlike \BERTS). Otherwise, the~optimal training configuration seems to be similar in both students.

    \begin{table}[h!t]
    \centering
    \footnotesize
    \begin{tabular}{lP{5.7cm}P{5.8cm}}
    \toprule
    & \multicolumn{2}{c}{explored values} \\
    \cmidrule(lr){2-3} parameter & \multicolumn{1}{c}{\LSTMS} & \multicolumn{1}{c}{\BERTS} \\
    \toprule
    $\eta$ 
      & \textbf{Adadelta ($\bm{\eta=1.0}$)}, \underline{Adam} with $\eta\in$ [$5\times10^{-3}$, $1.5\times10^{-3}$, \underline{$5\times10^{-4}$}, $1.5\times10^{-4}$, $5\times10^{-5}$, $1.5\times10^{-5}$, $5\times10^{-6}$] 
      & \textbf{\underline{Adam}} with $\eta\in$ [$5\times10^{-3}$, $1.5\times10^{-3}$, \underline{$\bm{5\times10^{-4}}$}, $1.5\times10^{-4}$, $5\times10^{-5}$, $1.5\times10^{-5}$, $5\times10^{-6}$] \\
    \hline
    $\eta$ scheduling & $E_w\in$ [\underline{\textbf{0}}, 5, 10, 15] + \underline{decay}/\textbf{no decay} & $E_w\in$ [0, 5, \textbf{10}, \underline{15}, 20] + \underline{\textbf{decay}}/no decay \\
    \hline
    $B$ & [\underline{32}, \textbf{50}, 128, 256, 512] & [32, 50, \underline{128},  \textbf{256}, 512] \\
    \hline
    embeddings & \textbf{word-level}/wordpiece + non-static/\textbf{multichannel} & word-level/\textbf{wordpiece} + \textbf{non-static}/multichannel \\
    \bottomrule
    \end{tabular}
    \caption{The~hyperparameter values explored on CoLA, one at a~time, from top to bottom. In bold are shown the~initial values. Underlined are the~best values (for embedding mode and type, the~best configuration is chosen separately for each task and is summarised elsewhere).}
    \label{tab:hparam-exploration}
    \end{table}

    Following the~initial hyperparameter exploration, the~best embedding mode and type configuration is identified for each task. Observing the~performance gap between the~two students, I~hypothesise that this may be due to the~word-level embeddings in \LSTMS~being more suitable than the~wordpiece embeddings in \BERTS.
    
    The~results of trying each embedding type combined with each embedding mode show that the~multichannel mode is generally preferred\footnote{I.e. it is helpful to use an~additional embedding matrix which is frozen during student training.}, and that the~best embedding type depends on the~task (more details in \autoref{sec:A-exploration-embed} in \autoref{chap:A-student-training}). 
    In particular, word2vec embeddings work slightly better for CoLA and SST-2, but are not preferred for Sara. This is likely due to Sara examples containing many mistyped words like ``yesyesyes'', which are treated as the~general \verb|UNKNOWN| word in word2vec, but are successfully broken down into smaller, meaningful units when using wordpieces. Thus, it may be preferable to use word2vec (or similar word-level embeddings) where the~language is expected to be mostly clean, free of unusual or mistyped words (formal and semi-formal domains), while wordpieces provide a~fallback alternative for informal domains.

    While both of the~2.4-million parameter students perform very well on SST-2 and Sara (on par with the~teacher), there continues to be a~gap on CoLA: the~teacher being far ahead, and \LSTMS~outperforming \BERTS.
    To reduce this gap and to explore the~effect of different student dimensions in general, I~systematically vary the~width and depth of each student -- that is, the~dimensionality of the~hidden layers and internal representations, and the~number of layers (encoder layers in \BERTS, BiLSTM layers in \LSTMS).
    On CoLA, the~\BERTS~is made up to 4x wider and 3x deeper, and \LSTMS~is made up to 5x wider and deeper. On SST-2 and Sara, to explore how small the~students can be to still achieve over 90\% of their teachers' score, \BERTS~is made up to 16x slimmer and 4x shallower, and \LSTMS~is made up to 32x slimmer (originally with just one BiLSTM layer, it cannot be more shallow). (For details of the~explored dimensions, see \autoref{tab:sizes-bert} and \autoref{tab:sizes-lstm} in \autoref{chap:A-student-training}; in particular, note that I~had to manually reduce the~learning rate for large \BERTS~models to prevent gradient explosion.)

    The~results of the~student size exploration (details in \autoref{sec:A-student-size}) show that model width is, on these three tasks, more important than model depth. In particular, the~performance gap between \LSTMS~and \BERTS~on CoLA is closed by increasing the~width of the~latter student (to roughly match the~\LSTMS's layer width). On SST-2 and Sara, making the~models slimmer affects their performance more than making them shallower. However, the~results on CoLA suggest that \LSTMS~may be too shallow for this difficult task and that using 2 and more hidden BiLSTM layers is beneficial (compare with \BERTS, which uses 5 hidden layers by default, and further increasing this does not help).
  }

  \section{Discussion and summary}{
    \begin{table}[h!bt]
      \centering
      \footnotesize
      \makebox[\textwidth][c]{\begin{tabular}{|l|l|l|l|l|c|c|c|c|}
        \hline
       &
        model  &
        dimensions &
        training &
        embed. &
        size &
        RPE &
        eval &
        test \\
        \hhline{|=|=|=|=|=|=|=|=|=|}
      \rowcolor{CoLA-l}
        &
        \BERTT &
        \begin{tabular}{@{}l@{}}$L=24$, $d_h=1024$, \\$d_I=4096$, $A=16$ \end{tabular} &
        \begin{tabular}{@{}l@{}}$B=36$, $\eta=5\times10^{-5}$, \\$E_w=0.3$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}piece\end{tabular} &
        340M &
        750ms &
        59.9 &
        54.6 \\
      \hhline{~--------}
      \rowcolor{CoLA-l}
        &
        \BERTS &
        \begin{tabular}{@{}l@{}}$L=5$, $d_h=816$, \\$d_I=3000$, $A=12$ \end{tabular} &
        \begin{tabular}{@{}l@{}}$B=128$, $\eta=7\times10^{-5}$, \\$E_w=15$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}word,\\multi\end{tabular} &
        76.4M &
        100ms &
        45.0 &
        29.8 \\
      \hhline{~--------}
      \rowcolor{CoLA-l}
      \multirow{-5}{*}{\rotatebox[]{90}{\centering CoLA}}  &
        \LSTMS &
        \begin{tabular}{@{}l@{}}$L=2$, $d_{LSTM}=600$,\\$d_{FC}=800$\end{tabular} &
        \begin{tabular}{@{}l@{}}$B=32$, $\eta=5\times10^{-4}$, \\$E_w=0$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}word,\\multi\end{tabular} &
        15.4M &
        2.3ms &
        44.2 &
        27.9 \\
        \hline
      \rowcolor{SST-2-l} &
        \BERTT &
        \begin{tabular}{@{}l@{}}$L=24$, $d_h=1024$, \\$d_I=4096$, $A=16$ \end{tabular} &
        \begin{tabular}{@{}l@{}}$B=36$, $\eta=5\times10^{-5}$, \\$E_w=0.3$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}piece\end{tabular} &
        340M &
        750ms &
        91.5 &
        93.1 \\
      \hhline{~--------}
      \rowcolor{SST-2-l}
        &
        \BERTS &
        \begin{tabular}{@{}l@{}}$L=5$, $d_h=204$,\\$d_I=750$, $A=3$\end{tabular} &
        \begin{tabular}{@{}l@{}}$B=128$, $\eta=5\times10^{-4}$, \\$E_w=15$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}word,\\multi\end{tabular} &
        2.42M &
        12ms &
        89.3 &
        87.8 \\
      \hhline{~--------}
      \rowcolor{SST-2-l}
      \multirow{-5}{*}{\rotatebox[origin=c]{90}{SST-2}} &
        \LSTMS &
        \begin{tabular}{@{}l@{}}$L=1$, $d_{LSTM}=300$,\\$d_{FC}=400$\end{tabular} &
        \begin{tabular}{@{}l@{}}$B=32$, $\eta=5\times10^{-4}$, \\$E_w=0$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}word,\\multi\end{tabular} &
        2.41M &
        1.0ms &
        91.2 &
        92.2 \\
        \hline
      \rowcolor{Sara-l}
        & \BERTT &
        \begin{tabular}{@{}l@{}}$L=24$, $d_h=1024$, \\$d_I=4096$, $A=16$ \end{tabular} &
        \begin{tabular}{@{}l@{}}$B=36$, $\eta=5\times10^{-5}$, \\$E_w=1$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}piece\end{tabular} &
        340M &
        750ms &
        87.5 &
        88.3 \\
      \hhline{~--------}
        \rowcolor{Sara-l}
        &
        \BERTS &
        \begin{tabular}{@{}l@{}}$L=5$, $d_h=204$,\\$d_I=750$, $A=3$\end{tabular} &
        \begin{tabular}{@{}l@{}}$B=128$, $\eta=5\times10^{-4}$, \\$E_w=15$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}piece\end{tabular} &
        2.43M &
        11ms &
        87.1 &
        86.4 \\
      \hhline{~--------}
      \rowcolor{Sara-l}
      \multirow{-5}{*}{\rotatebox[origin=c]{90}{Sara}} &
        \LSTMS &
        \begin{tabular}{@{}l@{}}$L=1$, $d_{LSTM}=300$,\\$d_{FC}=400$\end{tabular} &
        \begin{tabular}{@{}l@{}}$B=32$, $\eta=5\times10^{-4}$, \\$E_w=0$, decay\end{tabular} &
        \begin{tabular}{@{}l@{}}piece,\\multi\end{tabular} &
        5.90M &
        0.68ms &
        86.5 &
        86.4 \\
      \hline
      \end{tabular}}
      \caption{Essential information about the~students and teachers on each downstream task. The~model size is in millions of trainable non-embedding parameters. The~embedding type (``embed.'') is ``word'' (word2vec) or ``piece'' (wordpiece), the~mode is either ``multi'' (multichannel), or the~default non-static mode where not explicitly stated. The~inference runtime per example (RPE) is calculated by measuring the~time for processing the~entire evaluation set in batches of 256 (previously transformed from text into numerical form), then turned into time per single example and reported. Evaluation-set (``eval'') and test-set (``test'') scores are reported using the~appropriate metric (accuracy, MCC, $\text{F1}_{micro}$). The~student size differs between SST-2 and Sara due to the~higher dimensionality of wordpiece embeddings (1024) compared to word2vec (300).}
      \label{tab:best-students}
    \end{table}

    \autoref{tab:best-students} summarises the~hyperparameters chosen for each student on each task, and compares all models in terms of their size, prediction speed\footnote{Timed on a~laptop with an~Intel Core i7-6600U CPU, i.e. not using a~GPU.} and score.
    Clearly, CoLA is more difficult of a~task than SST-2 and Sara: In the~student size exploration, even models with over 100 million parameters achieve only below 75\% of the~teacher's score. As a~compromise between accuracy and model size, I~choose for analysis students that are 4.5x (\BERTS) and 22x (\LSTMS) smaller than the~teacher and both achieve similar scores.
    On SST-2 and Sara, on the~other hand, the~\mytilde2.4-million-parameter students, being \mytilde140x smaller than \BERTT, reach comparable accuracy. The~students can be even smaller while staying above 95\% of the~teacher's score: E.g. \LSTMS~on SST-2 can be 14,000x smaller (64x slimmer than the~2.4-million-parameter version), and \BERTS~can be 2000x smaller (3x slimmer and 2x shallower than the~2.4-million-parameter version).
    The~students are also much faster than the~teacher models, with \LSTMS~being particularly fast\footnote{\LSTMS~may be faster than \BERTS~partly due to different input feeding strategies; while for \BERTS~all input examples are padded to the~model's maximum sequence length of 128, for \LSTMS, I~only pad all examples within a~batch to the~length of the~longest example, which leads to more compact batches.} -- up to 1100x faster than \BERTT.
    
    There is evidence of model width being the~key dimension, with small model depths being sufficient for decent performance levels.
    Possibly, models like the~12- and 24-layer BERTs released by \citet{Devlin_2018} are unnecessarily deep for tasks like intent classification or even grammatical acceptability. Thus, making the~models shallower is one way of compressing and accelerating them (in line with \citet{Sanh_2019} who created well-performing BERT student shallower but not slimmer than BERT\textsubscript{Base}).

    There are several differences between \BERTS~and \LSTMS. Notably, the~LSTM student converges much faster, but works best with smaller minibatches, which makes training slower compared to large batches. The~BERT student is more sensitive to the~learning rate values and these need to be significantly reduced for larger \BERTS~sizes.
    Otherwise, the~models are not too sensitive to hyperparameter choices, and the~configuration chosen on CoLA works well when used in students trained on SST-2 and Sara.

    In knowledge distillation, the~teacher's knowledge enters the~student in the~top layer (where feedback is received during training, with error gradients ``trickling down'' into the~lower student layers).
    The~provision of trained embeddings to a~student creates the~opposite (and complementary) flow of knowledge: from the~bottom up, as the~knowledge captured in the~embeddings propagates into the~rest of the~model.
    I~showed that while both word-level and wordpiece embeddings work well with both student architectures, certain downstream tasks (here CoLA and SST-2) benefit from the~higher-quality word-level representations while others like Sara need the~flexibility of wordpiece embeddings.
    It would be interesting to see how well word-level embeddings fine-tuned as part of the~teacher model would perform.

    Besides leaving the~embedding layer to be further trained during knowledge distillation, I~observe the~usefulness of keeping another -- frozen -- copy of the~embeddings\footnote{This corresponds to the~multichannel embedding mode.}. In other words, the~student benefits from having access both to the~original embeddings and to the~embeddings trained as the~student learns.

    The~9 models described in \autoref{tab:best-students} -- one teacher and two students for each task -- are further analysed in the~remainder of this work.
  }
}

\chapter{Analysing the~models}{
  In this chapter, probing and prediction analysis are used to analyse, interpret and compare the~teacher and the~student models for each downstream task. 
  The~aim is to produce insights into the~nature of the~downstream tasks, the~models, and knowledge distillation.

  \section{Probing}{
    By probing, in this section, I~find that model initialisation plays a~key role in student learning; localise different linguistic skills in the~models; and point out possible limitations of the~probing suite.

    \citet{Tenney_2019b} recently showed that a~typical text processing pipeline can be identified in BERT.    
    In the~probing suite used here \citep{Conneau_2018}, the~pipeline steps are represented from the~simplest ones to the~most complex ones; from extracting surface properties of input such as sentence length (probing task \verb|Length|) to detecting broken semantics (tasks \verb|OddManOut| and \verb|CoordinationInversion|); re-visit \autoref{sec:probing-tasks} for more details on each probing task.
    Here, probing is used to trace language knowledge which enters models in various ways -- as pre-trained knowledge (in the~teacher BERT before fine-tuning), via trained embedding parameters (in both students), or the~knowledge flow from \BERTT~into students during knowledge distillation.
    While \citeauthor{Conneau_2018} apply probing only to the~last layers of their models, I~probe different layers\footnote{As discussed in \autoref{sec:implementation-details-probing}, in the~case of \LSTMS, I~am only able to probe the~final representations due to the~way LSTMs are implemented in PyTorch.} in order to also localise the~language knowledge within models (similar to \citeauthor{Tenney_2019b}).
    
    \begin{figure}[h!b]
      \centering
      \makebox[\textwidth][c]{\includegraphics[width=17.5cm]{../experiments/analysis/img/probing_teachers.pdf}}
      \cprotect\caption{Probing results for the~pre-trained BERT and the~teacher BERT models. Probing was applied to encoder layers 1, 6, 12, 18, and 24, and to the~embeddings (layer ``E'') extracted just before the~first encoder layer. The~two baselines, reasonably bounding the~expected model performance from below and from above, are the~majority-class baseline and the~human performance baseline; additionally, the~best model scores are shown (``Conneau'') -- all baselines as reported by \citet{Conneau_2018}.}
      \label{fig:probing-teachers}
    \end{figure}

    \begin{figure}[h!t]
      \centering
      \makebox[\textwidth][c]{\includegraphics[width=18cm]{../experiments/analysis/img/probing_students.pdf}}
      \caption{Probing results for the~best student models. For comparison, results achieved just by using the~embeddings (taken before student training) are shown as well (``embed.'') (the average pooling strategy was used to construct probing sentence encodings from the individual embeddings of a sentence's words/wordpieces). For \LSTMS, the~probing encodings were extracted only after the~last LSTM layer; for \BERTS, they were extracted after each encoder layer (1-5) and before the~first layer (``E''). The~majority-class baseline and the~human performance baseline are shown, same as in \autoref{fig:probing-teachers}.}
      \label{fig:probing-students}
    \end{figure}

    Probing results are shown in \autoref{fig:probing-teachers} (teacher models) and in \autoref{fig:probing-students} (students).
    In general, students achieve lower probing scores than their teachers,
    especially on the~difficult, semantical tasks (\verb|OddManOut|, \verb|CoordinationInversion|, partly \verb|BigramShift|), which require good sentence-level understanding,
    not just word-level knowledge found in embedding parameters.

    Several architectural differences between the~models are reflected in probing results.
    In the~deep teacher models, only the~last layers change in fine-tuning (compare fine-tuned teachers with the~pre-trained BERT\textsubscript{Large}), while the~earlier layers act like downstream-task-agnostic general feature extractors\footnote{Perhaps these layers could be frozen in order to make fine-tuning faster.}.
    In the~shallow \LSTMS, the~BiLSTM layer also likely serves as a~general feature extractor -- its probing score is comparable across the~downstream tasks, and is often better than that of the~downstream-task-specific last layer of \BERTS.
    The~performance gap between the~students on \verb|WordContent| may be linked to the~lack of residual connections in \LSTMS~-- in the~BERT student, these connections enable easy copying of input into higher layers.
    Lastly, the~recurrent LSTM student architecture may be more suited for order- and length-sensitive tasks (\verb|Length|, \verb|BigramShift|, \verb|CoordinationInversion|), as the~results on SST-2 and Sara show.

    Where the~downstream task does not require sophisticated linguistic skills (SST-2, partly also Sara), the~language knowledge is mostly lost or not acquired in both teachers and students\footnote{Only on \verb|WordContent|, the~CoLA teacher is outperformed by the~Sara teacher; I~attribute this to many Sara intents being recognisable by characteristic keywords (e.g., examples of the~intent \verb|affirm| typically contain ``yes'' or ``okay''), which motivates the~Sara teacher to learn to ``remember'' exact input words.}.
    On CoLA, on the~other hand, the~linguistic skills are mostly retained/acquired or even slightly improved in the~later layers of both teachers and students
    
    Additionally, \autoref{fig:probing-teachers} confirms that surface skills (\verb|Length|, \verb|WordContent|) are found in the~early teacher layers, syntactic skills (\verb|Depth|, \verb|TopConstituents|) are in the~middle layers, and semantic skills (\verb|OddManOut|, \verb|CoordinationInversion|) concentrate in the~final layers.
    % An~interesting case is the~\verb|BigramShift| task which, as previously discussed, is both syntactic and semantic, because disturbing the~word order in a~sentence typically disrupts both the~meaning and the~sentence structure structure. As a~result, this probing task is handled well by the~middle layers as well as the~later ones.
    
    \begin{figure}[h!tb]
      \centering
      \makebox[\textwidth][c]{\includegraphics[width=8.5cm]{../experiments/analysis/img/probing_students_scratch_selected.pdf}}
      \cprotect\caption{Probing results for selected downstream and probing tasks, comparing students initialised in the~standard way (with embeddings from word2vec) with students initialised randomly and trained from scratch. Bounding the~expected model performance from below and from above are again the~majority-class baseline and the~human performance baseline, same as in \autoref{fig:probing-teachers}.
      Additionally, the~performance of a~simple ``morphology-guessing'' model is shown, which transforms any input sentence into ``0'' or ``1'' depending on whether any of the~sentence's words end in ``d'' or not (for \verb|Tense|) or in ``s'' (for \verb|SubjNumber| and \verb|ObjNumber|).}
      \label{fig:probing-students-scratch-selected}
    \end{figure}

    The~results show important effects of initial ``provision of knowledge'' to students via trained embedding parameters.
    In \BERTS, often only the~bottom layers achieve good scores (see \verb|TopConstituents|, \verb|Tense|, \verb|SubjNumber| and \verb|ObjNumber| in \autoref{fig:probing-students})\footnote{This could be verified for the~LSTM student if it had more layers and if it was possible to probe all of them separately.}, which reflects the~knowledge ``leaking'' from the~embeddings up through the~model.
    This knowledge captured solely by the~embeddings is significant (see the~``embed.'' results in \autoref{fig:probing-students}), and \autoref{fig:probing-students-scratch-selected} shows that when it is not given to students before training\footnote{I.e. when all student parameters are initialised randomly from scratch.}, the~embedding-reliant skills (\verb|Tense|, \verb|SubjNumber|, and \verb|ObjNumber|) worsen, with the~bottom layers of \BERTS~no longer achieving good scores.
    In light of these results, and by showing that a~very simple rule-based morphology-guessing model\footnote{This model is based on the~observation that, in English, just knowing if \textit{any of the~words} in a~sentence are in the~plural form (dominantly marked by the~suffix ``-s/-es'') is a~decent proxy of whether \textit{the~subject/object} is in the~plural form, and similarly with verb tense (present/past) marked by the~suffix ``-d/-ed''. Note that such morphological information can be captured in word2vec, as observed by \citet{Gieske_2017}.} can achieve decent scores (``morph. guess'' in \autoref{fig:probing-students-scratch-selected}), I~argue that these tasks should be used carefully as indicators of semantic skills.    

    All in all, probing can provide useful insights but it is important to interpret the~results correctly: A~high score might not mean that the~model layer \textit{learnt} the~skill. Perhaps, knowledge was present before training either in that layer or in a~neighbouring model component. Last model layers are also misleading because they focus on task-specific knowledge, and only show linguistic skills if the~downstream task explicitly needs them (such as CoLA).
  }

  \section{Analysing the~models' predictions}{
    In this section, by inspecting the~models' evaluation-set predictions, I~observe that the~most sophisticated skills are not transferred well into the~students; that both students make similar mistakes; and that the~LSTM student can better mimic the~teacher.

    In inspecting the~predictions, both correctness and cofidence is considered\footnote{Certainly, a~confident incorrect prediction is not the~same as a~very unconfident decision which also happens to be incorrect.}.
    Here, I~define the~confidence of a~prediction as the~probability assigned to the~predicted class\footnote{For binary classification, the~minimum confidence is 0.5 and the~maximum is 1.0. A~possible limitation of this definition is that confidence may be generally lower on tasks with many classes, because softmax-produced probabilities are always above-zero for every single class.}.
    While the~analysis comprises mostly qualitative, manual inspection of sentences accompanied by predicted labels and prediction confidences (see \autoref{fig:prediction-analysis-example}), where possible, quantitative, aggregated results are also presented.
   
    \begin{figure}[h!b]
      \centering
      \makebox[\textwidth][c]{\includegraphics[width=17cm]{graphics/prediction-analysis-example1}}
      \caption{Example of the~interface used for inspecting the~predictions, in this case the~predictions of \LSTMS~on the~Sara task.}
      \label{fig:prediction-analysis-example}
    \end{figure}

    \begin{table}[t]
      \centering
      \footnotesize
      \makebox[\textwidth][c]{\begin{tabular}{|m{2cm}|m{10cm}|m{3.3cm}|}
      \hline
       & \textbf{point of focus} & \textbf{rationale} \\ 
      \hhline{|=|=|=|}
      \multirow{1}{2cm}{\textbf{individual models}} &
        P1: individual examples predicted correctly (hits) and incorrectly (mistakes); separately the~most confident and unconfident cases &
        \multirow{3}{3.5cm}[0.5em]{understanding individual students' strengths/weaknesses} \\ 
      \cline{2-2} & P2: average confidence: overall, separately on mistakes and hits & \\
      \hline
      \multirow{4}{2cm}[-1em]{\textbf{model differences}} &
        P3: comparing average confidence levels of models &
        \multirow{4}{3.5cm}[-1em]{understanding differences between students in terms of their skills and confidence patterns} \\ \cline{2-2}
       &
        P4: individual examples predicted correctly by only one student (incorrectly by the~other one) &
         \\ \cline{2-2}
       &
        P5: individual examples predicted confidently by only one student (unconfidently by the~other one) &
         \\ \cline{2-2}
       &
        P6: average overlap of the~mistakes and hits of different models &
         \\ \hline
      \multirow{3}{2cm}{\textbf{knowledge distillation}} &
        P7: individual examples predicted correctly by the~teacher and incorrectly by both students &
        \multirow{2}{3.5cm}{understanding the~skills not learned via distillation} \\ \cline{2-2}
       &
        P8: individual examples predicted confidently by the~teacher and unconfidently by both students &
     \\ \hline
      \end{tabular}}
      \caption{The~structure of the~prediction analysis: The~points of focus for each of the~three main areas in which insights are mined from the~model predictions.}
      \label{tab:prediction-analysis-structure}
    \end{table}

    \begin{table}[h!b]
      \centering
      \footnotesize
      \makebox[\textwidth][c]{\begin{tabular}{|c|m{15cm}|}
      \hline
      P1 &
      Confident hits: Easy examples from dominant classes (acceptable for CoLA, enter\_data for Sara; see \autoref{fig:class-balance} for detailed class distribution in each dataset).
      Unconfident hits/mistakes: Difficult examples; with mixed positive and negative words (SST-2); without characteristic keywords (Sara). 
      Confident mistakes: Examples with questionable labels, strong words opposing the~overall sentiment (SST-2); misleading keywords characteristic of an~incorrect intent (Sara); examples where recognising unacceptability requires semantic understanding (CoLA).
      \\ \hline
      P2 &
      On average, models are more confident on hits than on mistakes (see \autoref{fig:prediction-analysis-avg-confidence}).
      \\ \hline
      P3 &
      On average, \BERTT~is slightly more confident than either student (may be due to scoring more of the~[confident] hits, especially on CoLA) (see \autoref{fig:prediction-analysis-avg-confidence}).
      \\ \hline
      P4 &
      All examples are long and difficult, \BERTT~is unconfident as well as one or both students (not necessarily the~incorrect one).
      On Sara, several examples where \BERTT~better understands meaning and is not mislead by keywords, but one student is.
      \\ \hline
      P5 & 
      All the~examples are difficult, with all models often unconfident or even mistaken.
      On SST-2, the~\LSTMS~is unconfident often when the~teacher is unconfident (the~same correlation is not observed for \BERTS). \autoref{fig:confidence-correlation} quantitatively confirms this and shows moderate correlation between all model pairs' confidences on Sara (and mostly weak correlation on other downstream tasks).
      \\ \hline
      P6 &
      Students are relatively unique in their mistakes: On CoLA and SST-2, \mytilde60-70\% of their mistakes are shared by both students. On Sara, this sharing is above 80\%.
      The~architecturally different \LSTMS~learns to copy \BERTT's behaviour more closely than the~BERT student (mostly in terms of copying the~teacher's mistakes).
      [For detailed quantitative results, see \autoref{fig:hits-mistakes-overlap-all} in \autoref{chap:A-model-analysis}.]
      \\ \hline
      P7 &
      Very difficult examples; classified unconfidently (and mostly incorrectly) by both students, and mostly correctly by \BERTT.
      Overall, they demonstrate the~teacher's superiority on challenging sentences, e.g. recognising \textit{bill's story about sue and max's about kathy both amazed me.} as acceptable (CoLA) or \textit{you live around here?} as ask\_wherefrom (Sara).
      \\ \hline
      P8 &
      Similar to P7; complicated examples that require understanding of semantics (CoLA, Sara) and of mild sentiment possibly expressed in metaphors (SST-2), e.g.
      \textit{my heart is pounding me} (CoLA, unacceptable), \textit{fuck yeah!} (Sara, affirm; misclassified by both students as handleinsult).
      \\ \hline
      \end{tabular}}
      \caption{Main observations from prediction analysis, individually for each analysis task from \autoref{tab:prediction-analysis-structure}.}
      \label{tab:prediction-analysis-findings}
    \end{table}

    To give the~analysis a~logical structure, I~propose these three objectives, each realised as a~number of analysis tasks (see \autoref{tab:prediction-analysis-structure}):
    1) characterising the~teacher and student models individually;
    2) characterising the~differences between the~models;
    and 3) characterising knowledge distillation through its limitations.
    
    \begin{figure}[h!tb]
      \centering
      \makebox[\textwidth][c]{\includegraphics[width=13cm]{../experiments/analysis/img/confidence-correlation}}
      \caption{Correlation between the~prediction confidences of different models, measured as the~Pearson correlation coefficient $R$. The~``*'' marks statistically significant correlation (for $p < 0.05$).}
      \label{fig:confidence-correlation}
    \end{figure}

    \begin{figure}[h!tb]
      \centering
      \makebox[\textwidth][c]{\includegraphics[width=14.5cm]{../experiments/analysis/img/average-confidence}}
      \caption{The~average confidence of evaluation-set predictions. Standard deviation errorbars are shown.}
      \label{fig:prediction-analysis-avg-confidence}
    \end{figure}

    When choosing examples for inspection, I~select the~most extreme cases, i.e. taking the~most/least confident hits in P1 or the~examples with largest confidence gap between the~relevant models (P5, P8).
    Where such sorting is not possible, examples are selected randomly from all suitable ones (P4, P7).
    In all cases, 10 examples are selected for inspection in order to reasonably constrain the~task\footnote{This still means inspecting 360 sentences for P1: For each downstream task and each model, the~10 most confident hits and mistakes, as well as the~10 most unconfident hits and mistakes.}.
    In P4, examples are selected such that they are classified correctly by \BERTT\footnote{This is used as an~indicator of the~particular example being not too difficult to classify correctly, meaning that both students have a~reasonable chance of being correct, even though one of them still makes an~incorrect prediction.}.

    \autoref{tab:prediction-analysis-findings} summarises the~observations from each analysis task. In general, examples referred to as ``easy'' are mostly simple sentences on CoLA (\textit{the~witch poisoned the~children.}), sentences with clear sentiment on SST-2 (\textit{a gorgeous, witty, seductive movie.}), and sentences with clear keywords on Sara (\textit{the~bot speaks \textbf{spanish}}\footnote{Names of several languages appear extremely often in the~enter\_data intent examples.}). Difficult examples may require detailed understanding, e.g. recognising the~broken semantics in \textit{my heart is pounding me.} (CoLA).

    To further aid my understanding of predictions on CoLA, lastly, I~employ an~experimental approach: I~let all models classify numerous perturbed variants of 21 interesting CoLA sentences which were originally predicted correctly by \BERTT~but incorrectly by one or both students\footnote{The~complete list of the~sentences and their perturbed versions is in \autoref{tab:own-CoLA-diagnostics} in \autoref{chap:A-model-analysis}.}.
    This produces further evidence of students not being sensitive to valid/broken sentence semantics, see \autoref{tab:pred-anal-cola-case-studies}.
    In addition to ``allowing'' John to be a~tree, the~students are also found to be sensitive to concrete word choices where these should not matter, e.g. \textit{most of the~fruit is ripened.} is classified differently from \textit{most of the~fruit is spoiled.}

    \begin{table}[h!tb]
      \centering
      \footnotesize
      \begin{tabular}{lcccc}
      \toprule
      & label & \BERTT & \BERTS & \LSTMS \\
      \toprule
      \textit{my heart is pounding me. }& \xmark & \xmark~0.89 & \cmark~0.96 & \cmark~0.88 \\
      \textit{my heart is pounding.    }& \cmark & \cmark~0.98 & \cmark~0.97 & \cmark~0.98 \\
      \textit{my heart is beating me.  }& \xmark & \xmark~0.92 & \cmark~0.93 & \cmark~0.98 \\
      \textit{my heart is beating.     }& \cmark & \cmark~0.98 & \cmark~0.93 & \cmark~0.98 \\
      \textit{we believed john to be a~fountain in the~park.}  &\xmark & \xmark~0.92 & \cmark~0.97 & \cmark~0.98 \\
      \textit{we believed john to be a~bench in the~park.   }  &\xmark & \xmark~0.93 & \cmark~0.96 & \cmark~0.98 \\
      \textit{we believed john to be a~musician in the~park.}  &\cmark & \cmark~0.98 & \cmark~0.98 & \cmark~0.98 \\
      \bottomrule
      \end{tabular}
      \caption{Two example CoLA sentences, each followed by my own perturbed variants. \cmark~= acceptable, \xmark~= unacceptable. Confidences are shown next to model predictions.}
      \label{tab:pred-anal-cola-case-studies}
    \end{table}
  }

  \section{Summary}{
    Probing and prediction analyses were applied to the~teacher and student models, each shedding light on a~different aspect.

    While probing results show student models being less linguistically competent than their teachers, this does not automatically imply that the~students failed to properly learn what they should. Instead, I~point out that the~teachers (and, more generally, models initialised from trained parameters) can contain linguistic knowledge not needed for the~downstream task at hand.
    Additionally, observations from probing quantitatively confirm that CoLA as a~task requires many different linguistic capabilities, whereas SST-2 and Sara do not.
    As for differences between the~different student architectures, \LSTMS~may be more sensitive to the~notions of length and order in input sentences.
    As a~limitation of the~probing suite of \citet{Conneau_2018}, I~point out that three of the~tasks are handled relatively well even by simple bag-of-embedding models, as well as by trivial rule-based morphological models.

    The~mostly qualitative prediction analysis helped to characterise sentences that are easy or difficult for the~models.
    The~LSTM student was shown to better copy the~teacher's behaviour.
    Still, both students fail to acquire the~most sophisticated skills of their teachers -- notably, the~students exploit easy cues such as keywords, but have limited sense of sentence semantics.
  }
}

\chapter{Overall discussion, conclusions and future work}{
  In this chapter, findings from all of this work are broadly discussed, before overall conclusions and ideas for future research are presented.
  In the~discussion, I~focus on knowledge distillation in an~applied context where model size and speed is to be optimised.
  The~model analysis approaches are viewed as tools for understanding and subsequently improving knowledge distillation in practice.

  \section{Distilling BERT into tiny models}{
    With a~lot of recent research on knowledge distillation for compressing BERT, one should carefully recognise that different works have different objectives.
    One stream of work aims to retain all of BERT's accuracy \citep{Sanh_2019,Sun_2019a,Jiao_2019}, but the~size compression ratio is 2-10x, which may still produce models too large and slow for many applications.
    Another stream \citep{Tang_2019a,Tang_2019b} explores the~limits of model compression using knowledge distillation at the~cost of losing some of BERT's accuracy. 
    My work is closer to the~second stream, originating from previous attempts at heavily accelerating BERT \citep{Sucik_2019}.
    Adopting an~applied mindset, I~also focus on how the~distillation process can be tailored for individual tasks, instead of proposing a~universally well-performing student.
    While I~observe that one student configuration can work well across several tasks, several decisions are best made on a~case-by-case basis, as discussed in what follows.

    The~student model dimensions should reflect the~task at hand.
    In particular, in research and applications aiming to compress models as much as possible, it may be preferred to tailor the~student size individually to each task, as opposed to benchmarking a~fixed-size architecture on a~diverse suite like GLUE.
    I~showed that on easy tasks like SST-2 or Sara, a~student can be several thousand times smaller than the~BERT\textsubscript{Large} teacher and still retain over 95\% of the~teacher's score.
    However, CoLA, as a~complex linguistic task, requires students to be both wider and deeper, and still they only achieve \mytilde75\% of the~teacher's score.
    Moreover, I~observe that it is the~student width, not depth, that plays a~key role: 
    Increasing the~number of BiLSTM or Transformer layers beyond 1-3 generally does not improve scores, but accuracy degrades when the~layer width is below 200-300 (with CoLA requiring \mytilde2-3x wider representations).

    Besides the~student size, model initialisation from trained parameters is important -- as a~way of giving students language knowledge to start with.
    Such initialisation makes sense especially in the~lower layers, which 1) tend to learn task-agnostic features (thus, general trained parameters can be re-used across tasks), and 2) are located far from the~top layer through which the~teacher's knowledge ``enters'' (which makes learning in these lower layers more difficult).
    I~observe that the~choice of embeddings (word-level vs wordpiece) can improve performance and is also possible to reason about, given some knowledge of the~data\footnote{Word-level embeddings being more suitable for domains with decent language, wordpieces more useful where many unknown words (slang, mistyped and similar) are expected.}.
    While initialisation from trained parameters is most easily done in the~embedding layer, ideally, this would be extended to multiple early layers\footnote{This is not commonly done, as these parameters are specific to each architecture, unlike the~token-level embeddings, which are highly re-usable.}. 
    In the~future, probing could be used to explore the~flow of teacher's knowledge in distillation variants that force students to mimic internal teacher layers (e.g. \citet{Jiao_2019,Sun_2019a}).
    When possible, copying the~teacher's encoder layer parameters directly into the~student is also an~attractive option \citep{Sanh_2019};
    however, this requires the~student to be as wide as the~teacher, which can be undesirable.

    Last but not least, the~choice of student architecture is important.
    While most studies on distilling BERT work towards smaller BERT versions, there is no reason to believe that other student architectures should be inferior.
    While in my experiments both students tend to perform comparably well, I~observe the~BiLSTM student to be more sensitive to order and length phenomena.
    Recently, \citet{Huang_2020} showed that the~two architectures can complement each other -- by combining them into one, they improved on BERT's accuracy.
    In practical applications, inference speed can make one architecture preferred, perhaps because its concrete implementation is more optimised.
    For instance, LSTMs can process variable-size batches whereas BERT requires all inputs to be padded to a~fixed maximum sequence length; this way, LSTMs can process shorter inputs faster, but BERT can not.
  }

  \section{What can models tell us}{
    Coming up with an~idea for improvement; building a~new model; evaluating it on a~benchmark like GLUE -- these are the~typical steps shared by many works on improving language models.
    I~am of the~opinion that trained models can reveal a~lot about themselves and make suggestions for further iterative improvements. In this work, I~demonstrate the~use of two complementary analysis approaches with this aim.

    Probing tasks can be used beyond the~purposes they were originally meant for.
    \citet{Conneau_2018} propose tasks for quantifying the~language knowledge present in the~last layer of a~sentence encoder model.
    \citet{Tenney_2019a,Tenney_2019b} inspect individual model layers and localise different linguistic capabilities within BERT.
    I~combine the~above approaches and, additionally, use probing to ``track'' language knowledge as it enters and propagates through the~models before and during training.
    This way, I~characterise the~impact of initialising the~students' embedding layer from trained parameters, and the~loss of general language knowledge in the~top layers of the~pre-trained BERT when this is fine-tuned on concrete downstream tasks.

    Both probing and inspecting the~models' predictions helps to describe the~teacher's skills not learnt well by students.
    In particular, I~observe that semantic skills are diminished in the~students even when the~downstream task clearly requires them (CoLA).
    As a~remedy, distilling this complex knowledge from the~teacher's intermediate layers \citep{Jiao_2019,Sun_2019a} could be tried, with the~effects monitored by student probing.

    Probing can motivate task-specific model adjustments.
    On CoLA, which is found to leverage complex linguistic skills from later model layer, the~students can be made deeper to facilitate the~learning of complex, semantic representations.
    On Sara, inspecting the~model's predictions reveals tendency for learning characteristic keywords.
    Depending on whether this behaviour is desirable or not, residual connections in students can be added/removed to make internal input-copying easier or harder.

    Considering the~confidence of predictions provides richer insights into model behaviour.
    I~observe that unconfident predictions correspond to challenging examples, whereas confident predictions are made on easy examples (classified correctly) and on ``tricky'' misclassified examples where models are misled often by a~single word.
    Calculating the~correlation between two models' prediction confidences can quantify the~extent to which models behave similarly in their perception of examples as easy or difficult.
    This way, I~observe that the~teacher and student models diverge the~most on CoLA and SST-2, but behave similarly on Sara examples.

    Both probing and prediction analysis have their limitations.
    Correct interpretation of probing results is difficult: A~high probing score does not imply that the~model actively learnt the~skill in question. 
    Instead, the~knowledge may have been present in trained embeddings, or acquired from the~transfer dataset which reflects the~teacher's knowledge but not necessarily the~task's needs.
    Furthermore, just because a~probing task is aimed for measuring a~specific skill, it may not measure this well -- for instance, a~semantic task may still heavily rely on easy lexical cues, or may represent only a~very narrow part of general semantic knowledge.
    Inspecting a~model's predictions has its own caveats -- in particular, it is difficult to grasp why exactly a~model misclassifies a~given sentence.
    While I~show that this can be sometimes clarified by testing the~model on numerous hand-crafted perturbations of the~original sentence, this approach is extremely laborious.
  }

  \section{Conclusions}{
    In this work, I~have explored the~use of teacher-student knowledge distillation for compressing the~large BERT language model into architecturally different, smaller models, separately on different sentence-classification tasks.
    With the~aim of extreme model compression, I~adopt the~approach of first fine-tuning BERT on a~specific task and subsequently distilling it into student models.

    My findings show that easier tasks like SST-2 (binary sentiment classification from movie reviews) and Sara (57-way intent classification of human messages from human-bot conversations) can be successfully handled by student models several thousand times smaller and faster than the~BERT\textsubscript{Large} teacher model.
    However, these tasks are very easy to start with, and applying BERT to them may be questioned.
    On the~very challenging task of linguistic acceptability judgement (CoLA), standard knowledge distillation cannot bridge the~gap between the~teacher and the~students, and a~more sophisticated way of knowledge transfer may be needed, especially to help the~students acquire semantic skills.

    Working with different student architectures and on different downstream tasks is relatively easy.
    In particular, roughly the~same hyperparameter configuration can be re-used.
    However, the~students' depth is best adjusted on each task (the~more complex the~task, the~deeper the~students), as is the~choice of pre-trained embeddings (word-level vs wordpiece).
    Both a~bidirectional LSTM student and a~down-scaled BERT student are found to work similarly well, with the~former one learning to copy the~teacher's behaviour more closely, despite the~architectural disparity.

    I~show that probing the~models for specific linguistic knowledge as well as inspecting the~models' predictions can be used to mine various insights about the~models, the~tasks, and about knowledge distillation.
    While the~first approach quantitatively measures different linguistic skills possessed by a~model, the~latter approach can provide useful concrete examples for further analysis.
    In general, both students are found to lack especially the~complex semantic understanding possessed by the~teacher, and challenging sentences are presented on which the~teacher succeeds but the~students fail.
    Further, probing is shown to usefully trace the~different sources of knowledge in models, producing insights which can be used to adjust the~student architecture and initialisation.

    Overall, I~have produced useful insights both by using knowledge distillation in different settings and by analysing the~trained teacher and student models.
  }

  \section{Directions for future work}{
    While the~approach of applying knowledge distillation to large Transformers is new and many things are yet to be properly researched, my focus is primarily on analysing the~technique and the~models it produces, the~main aim being better understanding.

    As the~biggest downside of prediction analysis, I~identify my uncertainty in identifying why a~model misclassified a~given sentence.
    While I~show that this can be partly addressed by perturbing the~sentence and effectively testing different hypotheses, the~approach is very laborious.
    Naturally, being able to easily and quickly interpret a~model's predictions is desirable.
    In future work, inspecting predictions could hugely benefit from automatically generated ``saliency maps'', i.e. highlighting the~input tokens which are most responsible for the~particular prediction outcome.
    In practice, such saliency highlighting can be achieved either by using attentional weights like in the~original self-attention \citep{Lin_2017}, or, where the~student doesn't use attention, by more general methods (several of them recently built into an~NLP model interpretation toolkit by \citet{Wallace_2019}).
    Additional automated approaches include masking out one or more words at a~time and scoring such incomplete sentences, or, with sequential models such as LSTMs, scoring progressively larger parts of the~input to observe how the~model's beliefs evolve while ``reading'' the~sentence.
    
    Observing that probing can be used for effectively ``tracking'' the~flow of language knowledge, one promising research direction is making such tracking more systematic.
    Importantly, creating a~proper methodology around the~use of probing tasks is desirable. Such methodology should facilitate more careful interpretation of probing scores, in particular by controlling for language knowledge which is present in the~model for reasons other than the~hypothesised one (e.g. as residual knowledge from pre-trained parameters).
    It is desirable to know how and why certain knowledge was acquired by a~model, not just where and how much of this knowledge is possessed by the~model.
    As another improvement of the~probing approach, it could be applied not just to layer outputs, but also to the~self-attention heads' outputs in Transformer models. This could either use existing general probing suites like that of \citet{Conneau_2018}, or adopt a~more bespoke approach similar to \citet{Clark_2019} who probe the~heads for relational language knowledge.

    While the~probing tasks used today rely on classification tasks\footnote{For instance, a~model is tested on its ability to distinguish between the~past and present tense of a~sentence's main verb.}, a~different approach to probing may be to quantify the~general sensitivity of a~model to a~particular \textit{concept} (such as the~concept grammatical tense).
    For this purpose, the~recently proposed approach of \citet{Kim_2017} could be used, which enables measuring any neural model's sensitivity to any concept representable by a~collection of input examples.
    In the~NLP domain, several concepts are especially easy to gather examples for: the~concept of sentence length (exemplified perhaps by a~collection of long sentences), the~concept of sentence type (e.g. a~collection of interrogative sentences, identified automatically by the~final ``?''), the~concept of tense (curated with the~help of an~automatic lexical parser), the~concept of formal or informal language (as warranted by a~particular text source), etc.
    Ideally, this would enable probing tasks to be more easily extended beyond linguistic skills. Additionally, it is interesting to know a~model's general sensitivity to, say, the~concept of tense, and compare this with the~model's score on a~binary tense-classification task.
    
    Naturally, further exploration could focus more directly on improving knowledge distillation as such.
    In particular, since student training takes long on the~large transfer datasets, it would be interesting to try varying the~amount of augmentation data while observing not just the~students' scores on downstream tasks, but also the~various language knowledge acquired.
    Additionally, I~observe that the~GPT-2-generated augmentation sentences (especially on SST-2 and Sara) are often non-sensical (e.g. \textit{what management nyy} or \textit{done str prevents them}). Generating more credible input examples -- by fine-tuning GPT-2 for longer, or by scoring the~generated sentences for grammaticality by another model -- could improve knowledge distillation. The~importance of the~presence of the~original training data in the~transfer dataset is also unclear; this could be reduced or, on the~other hand, amplified (by including the~data multiple times), while observing the~effects of such manipulation.

    Finally, other model compression techniques can be explored, similarly to knowledge distillation.
    One attractive approach is model pruning which removes individual weight connections or neurons \citep{Han_2015,Sajjad_2020}, or even entire attentional heads \citep{Michel_2019} or model layers \citep{Mao_2020}, often without significantly affecting the~model's accuracy.
    The~question is if and what linguistic skills are lost in this process, and whether these were useful at all in the~given context, or were correctly identified as spurious during the~pruning procedure.
  }
}

\bibliographystyle{apalike}
\bibliography{s1513472-minf2}

\appendix
\chapter{Datasets}{
  \label{chap:A-datasets}
  \begin{center}
    \begin{longtable}{p{0.3\textwidth}|p{0.4\textwidth}|p{0.3\textwidth}}
    \hline
    intent name & description & example \\ 
    \hline
    \endfirsthead
    
    \hline
    \multicolumn{3}{c}%
    {\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
    \hline
    \endhead

    \hline 
    \multicolumn{3}{r}{\textit{Continued on next page}} \\
    \endfoot
    \endlastfoot

    \begin{spverbatim}affirm\end{spverbatim} & affirmative response & \begin{spverbatim}yes please!\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_builder\end{spverbatim} & asking Sara who built her & \begin{spverbatim}who developed you\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_channels\end{spverbatim} & asking Sara about the~messaging channels that Rasa tools support & \begin{spverbatim}what chat channels does rasa uses\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_community_size\end{spverbatim} & asking Sara about the~size of the~Rasa contributor community & \begin{spverbatim}Is the~community large?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_
    differencecorenlu\end{spverbatim} & asking Sara about the~difference between two major components of the~Rasa tools: Rasa NLU and Rasa Core & \begin{spverbatim}what is the~difference between core and nlu?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_languages\end{spverbatim} & asking Sara about the~languages supported by Rasa tools & \begin{spverbatim}do you support french ?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_opensource\end{spverbatim} & asking Sara if Rasa products are open source & \begin{spverbatim}are you full open source\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_platform\end{spverbatim} & asking Sara about the~Rasa Platform product & \begin{spverbatim}tell me what is platform\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_python_version\end{spverbatim} & asking Sara about the~version of Python supported by Rasa tools & \begin{spverbatim}which python version\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_slots\end{spverbatim} & asking Sara about \textit{slots}, a~concept in Rasa tools for holding human-provided contextual information during conversations & \begin{spverbatim}what do you mean by slots?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_tutorials\end{spverbatim} & asking Sara about tutorials on using Rasa tools & \begin{spverbatim}is there a~tutorial for this?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_voice\end{spverbatim} & asking Sara about the~possibility to create a~voice assistant using Rasa & \begin{spverbatim}you have speech recognition?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_faq_what_is_forum\end{spverbatim} & asking Sara about the~online Rasa community forum & \begin{spverbatim}what can I~post in the~forum?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_how_contribute\end{spverbatim} & asking Sara how one can contribute to the~Rasa open-source project & \begin{spverbatim}How can I~add code to Rasa\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_howbuilt\end{spverbatim} & asking Sara how she was built & \begin{spverbatim}so how were you made?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_howdoing\end{spverbatim} & asking Sara how she is doing & \begin{spverbatim}Hows it going\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_howold\end{spverbatim} & asking Sara about her age & \begin{spverbatim}do you know how old you are?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_isbot\end{spverbatim} & asking Sara if she is a~bot & \begin{spverbatim}are you really a~bot\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_languagesbot\end{spverbatim} & asking Sara about the~languages she can speak & \begin{spverbatim}how many languages are you fluent in?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_question_in_forum\end{spverbatim} & asking Sara a~question about the~Rasa community forum & \begin{spverbatim}how can I~leave a~query in the~forum?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_restaurant\end{spverbatim} & asking Sara to recommend a~restaurant & \begin{spverbatim}Where should I~eat?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_time\end{spverbatim} & asking Sara about the~time & \begin{spverbatim}tell me the~time it is.\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_weather\end{spverbatim} & asking Sara about the~weather & \begin{spverbatim}excellent - is it hot in Berlin?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_whatismyname\end{spverbatim} & asking Sara to tell the~person's name & \begin{spverbatim}can you tell me my name?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_whatisrasa\end{spverbatim} & asking Sara what Rasa is & \begin{spverbatim}OK can u brief me Abt rasa\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_whatspossible\end{spverbatim} & asking Sara about the~things she can do/help with & \begin{spverbatim}how u can help me\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_when_next_event\end{spverbatim} & asking Sara about the~next scheduled Rasa community event & \begin{spverbatim}what date is the~next community event?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_wherefrom\end{spverbatim} & asking Sara where she is from & \begin{spverbatim}where did you grow up?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_which_events\end{spverbatim} & asking Sara about the~current Rasa community events & \begin{spverbatim}what sort of social events are we throwing?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_whoami\end{spverbatim} & asking Sara who the~human person is & \begin{spverbatim}tell me who I~am?\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_whoisit\end{spverbatim} & asking who is it (like on the~phone) & \begin{spverbatim}who am i talking to\end{spverbatim} \\
    \hline
    \begin{spverbatim}ask_why_contribute\end{spverbatim} & asking Sara about the~reasons to contribute to Rasa & \begin{spverbatim}Why should I~contribute to your code?\end{spverbatim} \\
    \hline
    \begin{spverbatim}bye\end{spverbatim} & ending a~conversation with Sara by saying bye & \begin{spverbatim}take care\end{spverbatim} \\
    \hline
    \begin{spverbatim}canthelp\end{spverbatim} & telling Sara she cannot help with what is needed & \begin{spverbatim}i guess you can't help me then\end{spverbatim} \\
    \hline
    \begin{spverbatim}contact_sales\end{spverbatim} & asking Sara about ways to contact the~Rasa sales team & \begin{spverbatim}i want to talk to sales\end{spverbatim} \\
    \hline
    \begin{spverbatim}deny\end{spverbatim} & provide a~negative, denying response to Sara & \begin{spverbatim}no sorry\end{spverbatim} \\
    \hline
    \begin{spverbatim}enter_data\end{spverbatim} & providing information asked for by Sara & \begin{spverbatim}the~assistant is in dutch\end{spverbatim}, or \begin{spverbatim}my name is __PERSON_NAME__\end{spverbatim} \\
    \hline
    \begin{spverbatim}greet\end{spverbatim} & saying hi to Sara & \begin{spverbatim}hey let's talk\end{spverbatim} \\
    \hline
    \begin{spverbatim}handleinsult\end{spverbatim} & telling an~insult to Sara & \begin{spverbatim}i hate your dumb face\end{spverbatim} \\
    \hline
    \begin{spverbatim}how_to_get_started\end{spverbatim} & asking Sara how one can get started with Rasa tools & \begin{spverbatim}how to build a~chatbot\end{spverbatim} \\
    \hline
    \begin{spverbatim}human_handoff\end{spverbatim} & asking to be put through to a~human instead of the~Sara bot & \begin{spverbatim}let me speak with a~real person please\end{spverbatim} \\
    \hline
    \begin{spverbatim}install_rasa\end{spverbatim} & asking Sara about installing Rasa & \begin{spverbatim}i need help setting up\end{spverbatim} \\
    \hline
    \begin{spverbatim}next_step\end{spverbatim} & asking Sara to proceed to the~next step & \begin{spverbatim}next step please\end{spverbatim} \\
    \hline
    \begin{spverbatim}nicetomeeyou\end{spverbatim} & saying to Sara it is nice to meet her & \begin{spverbatim}Good to meet you!\end{spverbatim} \\
    \hline
    \begin{spverbatim}nlu_generation_tool_
    recommendation\end{spverbatim} & asking Sara about tools that can be used to generate more NLU training data (intent examples like these) & \begin{spverbatim}i need more nlu data\end{spverbatim} \\
    \hline
    \begin{spverbatim}nlu_info\end{spverbatim} & asking Sara about the~Rasa NLU tool & \begin{spverbatim}what is a~intent?\end{spverbatim} \\
    \hline
    \begin{spverbatim}out_of_scope\end{spverbatim} & an~out-of-scope message not falling into any of the~other intent categories & \begin{spverbatim}how to climb the~tree?\end{spverbatim} \\
    \hline
    \begin{spverbatim}pipeline_recommendation\end{spverbatim} & asking Sara about the~pipeline configuration used when building bots using Rasa tools & \begin{spverbatim}what pipeline should i use?\end{spverbatim} \\
    \hline
    \begin{spverbatim}rasa_cost\end{spverbatim} & asking Sara about the~price of Rasa products & \begin{spverbatim}is rasa core paid?\end{spverbatim} \\
    \hline
    \begin{spverbatim}react_negative\end{spverbatim} & negative reaction (typically in response to Sara asking how the~person is feeling) & \begin{spverbatim}so sad :(\end{spverbatim} \\
    \hline
    \begin{spverbatim}react_positive\end{spverbatim} & positive reaction (typically in response to Sara asking how the~person is feeling) & \begin{spverbatim}you are cool man\end{spverbatim} \\
    \hline
    \begin{spverbatim}signup_newsletter\end{spverbatim} & asking Sara about signing up for a~newsletter & \begin{spverbatim}i want on that dope newsletter\end{spverbatim} \\
    \hline
    \begin{spverbatim}source_code\end{spverbatim} & asking Sara about her source code & \begin{spverbatim}your code please\end{spverbatim} \\
    \hline
    \begin{spverbatim}switch\end{spverbatim} & asking Sara about switching from a~competitor tool to Rasa & \begin{spverbatim}How to migrate from DialogFlow to Rasa?\end{spverbatim} \\
    \hline
    \begin{spverbatim}technical_question\end{spverbatim} & asking Sara an~assorted technical questions & \begin{spverbatim}do you have docker image for rasa?\end{spverbatim} \\
    \hline
    \begin{spverbatim}telljoke\end{spverbatim} & asking Sara to tell a~jok & \begin{spverbatim}say a~funny joke\end{spverbatim} \\
    \hline
    \begin{spverbatim}thank\end{spverbatim} & thanking Sara & \begin{spverbatim}amazing, thanks\end{spverbatim} \\
    \hline
    \caption{A complete list of the~intents found in the~Sara dataset.}
    \label{tab:sara-intent-list}
    \end{longtable}
  \end{center}

  \begin{figure}[h!tb]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=23cm,angle=-90]{../experiments/analysis/img/class-balance.pdf}}
    \caption{Class distribution in the~different downstream task datasets; not that this distribution is roughly the~same across the~different portions of each dataset.}
    \label{fig:class-balance}
  \end{figure}
}

\chapter{Student hyperparameter exploration}{
  \label{chap:A-student-training}
  \input{hparam-exploration}
}

\chapter{Details of model analysis}{
  \label{chap:A-model-analysis}
  \begin{figure}[h!tb]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=13.5cm]{../experiments/analysis/img/probing_students_scratch.pdf}}
    \caption{Probing results comparing students initialised in the~standard way (with embeddings from word2vec) with students initialised randomly and trained from scratch. Bounding the~expected model performance from below and from above are again the~majority-class baseline and the~human performance baseline, as reported by \citet{Conneau_2018}.}
    \label{fig:probing-students-scratch}
  \end{figure}
  \begin{figure}[h!tb]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=11cm]{../experiments/analysis/img/hits-mistakes-overlap-all}}
    \caption{Overlap of models' evaluation-set mistakes and hits. Each cell shows the~fraction of hits or mistakes shared by models M1 and M2, as a~percentage of the~total hits or mistakes made by model M1.}
    \label{fig:hits-mistakes-overlap-all}
  \end{figure}

  \include{own-CoLA-diagnostics}
}

\end{document}
