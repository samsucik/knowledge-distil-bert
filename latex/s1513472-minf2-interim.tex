\documentclass[bsc,frontabs,twoside,singlespacing,parskip,deptreport]{infthesis}

\usepackage[round]{natbib}
\usepackage[hidelinks,colorlinks,allcolors=blue]{hyperref}

\usepackage{graphicx}

\usepackage{textcomp} % for text tilde

% Nice references including the words Figure, Section, etc
\renewcommand*{\chapterautorefname}{Chapter}
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\renewcommand*{\figureautorefname}{Fig.}
\renewcommand*{\tableautorefname}{Tab.}
\newcommand{\algorithmautorefname}{Alg.}
\renewcommand{\equationautorefname}{Eq.}

% Change font family and size in captions
\DeclareCaptionFont{captionfont}{\small\fontseries{n}\fontfamily{phv}\selectfont}
\captionsetup[table]{labelsep=period,font=captionfont,justification=centering}
\captionsetup[figure]{labelsep=period,font=captionfont,justification=centering}


%
% TABLES
%

\usepackage{multirow} % multi-row and multi-column table cells
\usepackage{makecell} % line breaks inside cells with \thead{} and \makecell{}
\usepackage{longtable} % allow table to span multiple pages

% Globally setting the vertical padding in tables
\renewcommand{\arraystretch}{1.1}

% Spacing around lines in tables
\def\abovestrut#1{\rule[0in]{0in}{#1}\ignorespaces}
\def\belowstrut#1{\rule[-#1]{0in}{#1}\ignorespaces}
\def\abovespace{\abovestrut{0.17in}}
\def\aroundspace{\abovestrut{0.17in}\belowstrut{0.10in}}
\def\belowspace{\belowstrut{0.10in}}


% For minipages with multiple figures, each with its own subcaption
\usepackage{subcaption}

\begin{document}

\title{\vspace{-5.0cm} \centering{\includeshield} \vspace{1cm} \\ Learning small sentence classifiers from BERT using teacher-student knowledge distillation}

\author{Sam Su\v{c}\'ik}

\course{Master of Informatics}
\project{\vspace{3cm}{\bf MInf Project (Part 2) Report}}

\date{2020}

\abstract{
  
}

\maketitle

\section*{Acknowledgements}{
  Thanks to Steve and Vova, to Ralph Tang and to Sl\'avka.
}

{
  \hypersetup{linkcolor=black}
  \tableofcontents
}

\chapter{Introduction}{
  % what's the problem, 
  \section{Motivation}{
    \begin{itemize}
      \item After the deep learning hype started, NLP went through an era of LSTMs. Since 2017, the area has been becoming dominated by Transformers.
      \item As newer and bigger Transformer-based models were proposed in 2018 and 2019, improving on the SOTA, it was becoming clearer that their big size and low speed was rendering them difficult to use (both train and deploy) in practice outside of research labs.
      \item Recently, we've seen various early attempts at making Transformers -- in particular BERT -- smaller by removing attentional heads \citep{Michel_2019}, quantisation and pruning \citep{Cheong_2019, Sucik_2019}. In terms of actually down-sizing and accelerating the models, knowledge transfer using teacher-student knowledge distillation has led to the most attractive results \citep{Mukherjee_2019,Tang-et-al-2019a,Jiao_2019,Sanh_2019}.
      \item However, these studies focus on showing that knowledge distillation works well. Important questions about the nature of this technique and how it interacts with properties of the teacher and student models remain unexplored.
    \end{itemize}
  }
  
  % what I tried to address, 
  \section{Aims}{
    Since NLP tasks are often possible for humans to reason about, this setting creates an opportunity for trying to understand and interpret various characteristics of knowledge distillation, in this case in the context of Transformer models.

  }
  
  % what I did
  \section{Contributions}{}
}

\chapter{Background}{
  % NLP (sentence classification), transformers, knowledge distillation, model understanding (probing)
}

\chapter{Datasets}{
  % GLUE: CoLA, SST-2; Sara (anonymised!), probing tasks. Other alternatives considered.
}

\chapter{Implementation}{
  % pytorch-transformers, d-bert, SentEval, Rasa. own pieces. architecture. choices made. initial hparams.
}

\chapter{Student tuning}{
  % hparam tuning, size tuning
  % results: best config, student size differences between datasets
}

\chapter{Analysing the students}{
  % analysing mistakes, probing
}

\chapter{Overall discussion and conclusions}{}

\chapter{Future work}{}

\chapter{Notes}{
  \section{Distillation techniques}{
    \citet[p. 13]{Papamakarios-2015} point out that mimicking teacher outputs (e.g. with cross-entropy loss) can be taken to next level by mimicking the derivatives of the loss w.r.t. inputs (i.e. including in the KD loss function also this term: $\frac{\partial \mathbf{o}_{student}}{\partial \mathbf{x}} - \frac{\partial \mathbf{o}_{teacher}}{\partial \mathbf{x}}$), the additional loss term being calculated using the R technique (Pearlmutter, 1994).

    \citet{Sau-Balasubramanian-2016} show that learning from noisy logits helps (adding the noise is very simple).

    \citet{Kim-Rush-2016} observe that KD and weight pruning are orthogonal (can be used together), and that mimicking top-most hidden layer outputs (instead of outputs themselves) doesn't provide improvements previously reported.

    \citet{Huang-Wang-2017} propose method for matching neuron activation distributions of teacher and student (only suitable for same teacher/student architecture?). \citet{Heo-et-al-2018} do a similar thing but try to match the activation boundaries of neurons.
  }

  \section{Knowledge distillation -- assorted}{
    \citet{Zharov-et-al-2018} use KD of DNNs into decision forests for interpretability.

    \citet{Mirzadeh-et-al-2019} show that a large teacher cannot teach too small students, and that adding intermediate "teacher assistants" helps.
  }

  \section{Datasets}{
    Unsuitable:
    \begin{enumerate}
      \item \href{https://github.com/microsoft/CNTK/tree/master/Examples/LanguageUnderstanding/ATIS}{ATIS}: too easy (see \href{https://github.com/nghuyong/rasa-nlu-benchmark#result}{here}). Rasa version \href{https://github.com/nghuyong/rasa-nlu-benchmark}{here}.
      \item \href{https://github.com/sebischair/NLU-Evaluation-Corpora}{AskUbuntu, Chatbot and Web Applications (all from TU Munich): too small (max. 206 datapoints)}. Rasa version \href{https://github.com/nghuyong/rasa-nlu-benchmark}{here}.
      \item \href{https://github.com/snipsco/nlu-benchmark/tree/master/2016-12-built-in-intents}{SNIPS}: too easy (see \href{https://github.com/nghuyong/rasa-nlu-benchmark#result}{here}). Rasa version \href{https://github.com/nghuyong/rasa-nlu-benchmark}{here}.
    \end{enumerate}

    Suitable:
    \begin{enumerate}
      \item \href{https://fb.me/multilingual_task_oriented_data}{FB's Multilingual Task Oriented Dataset}: F1 0.99 by supervised embeddings (very easy, but perhaps usable). Rasa version \href{https://github.com/nghuyong/rasa-nlu-benchmark}{here}.
      \item \href{https://nyu-mll.github.io/CoLA/}{CoLA} \citep{CoLA-paper}, needs processing into Rasa format.
      \item \href{https://nlp.stanford.edu/sentiment/index.html}{SST} \citep{SST-paper}, needs processing into Rasa format. 5-way classification may be too hard (accuracy \texttildelow{}50\%), 2-way much easier.
      \item \href{https://cogcomp.seas.upenn.edu/Data/QA/QC/}{TREC question-type classification} \citep{TREC-paper}, needs processing into Rasa format. 6-way classification (abbreviation, entity, description, human, location, numeric), also has another (more fine-grained) level of categories.
    \end{enumerate}

    Maybe suitable:
    \begin{enumerate}
      \item \href{https://github.com/xiul-msr/e2e_dialog_challenge}{Microsoft Dialogue Challenge} \citep{MDC-paper} needs processing into Rasa format. Also, no results using Rasa.
      \item \href{http://fb.me/semanticparsingdialog}{TOP} \citep{TOP-paper} needs processing into Rasa format. Also, no results using Rasa. Intents are hierarchical, would need to take only the top-most intent.
      \item \href{https://github.com/allenai/scicite}{SciCite} \citep{SciCite-paper} needs processing into Rasa format. Also, no results using Rasa.
    \end{enumerate}

    Probing/evaluation:
    Started by Shi et al. (2016) and Adi et al. (2017)?
    \begin{enumerate}
      \item \href{https://github.com/nyu-mll/jiant/tree/master/probing/data}{Google's edge probing} \citep{Tenney-et-al-2019-1} for evaluating span representations on 9 tasks closely following classical NLP pipeline (data not freely accessible!)
      \item \href{https://github.com/facebookresearch/SentEval/tree/master/data/probing}{FB's probing} \citep{Conneau-et-al-2018} used to evaluate entire sentence embeddings (10 tasks from sentence length to semantics)
      \item \href{https://github.com/facebookresearch/SentEval}{FB's SentEval} \citep{SentEval-paper} is meant for evaluating trained sentence encoders (i.e. not meant as downstream tasks that encoders should be fitted to), but it curates interesting existing datasets.
    \end{enumerate}
  }

  \section{Plans for MVP}{
    Let's distill BERT into a smaller BERT.

    Code: pytorch-transformers (re-using code for DistilBERT), no Rasa for now.

    Dataset: CoLA (because pytorch-transformers offers easy feeding of GLUE datasets).
  }
}

\bibliographystyle{apalike}
\bibliography{s1513472-minf2}

\appendix

\end{document}
